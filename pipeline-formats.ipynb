{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bccc1d6-cf1e-4547-bd51-7d0429d9d4bc",
   "metadata": {},
   "source": [
    "# ECMWF Observation Processing Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline converts meteorological observations from native formats (NetCDF, HDF5) to WMO BUFR format using Python and ecCodes. It's designed for operational use at ECMWF and follows WMO standards for observation encoding.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Multi-format Input Support**: NetCDF, HDF5, NetCDF4\n",
    "- **Multiple Observation Types**: Surface, radiosonde, satellite observations\n",
    "- **Quality Control**: Built-in QC checks for data validation\n",
    "- **Batch Processing**: Process entire directories of files\n",
    "- **Flexible Configuration**: JSON-based configuration system\n",
    "- **BUFR Validation**: Automatic validation of generated BUFR files\n",
    "- **Robust Error Handling**: Comprehensive logging and error management\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install ecCodes library (system dependency)\n",
    "# On Ubuntu/Debian:\n",
    "sudo apt-get install libeccodes-dev\n",
    "\n",
    "# On CentOS/RHEL:\n",
    "sudo yum install eccodes-devel\n",
    "\n",
    "# On macOS with Homebrew:\n",
    "brew install eccodes\n",
    "```\n",
    "\n",
    "### Python Dependencies\n",
    "\n",
    "```bash\n",
    "pip install eccodes-python netcdf4 h5py numpy pandas\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Command Line Interface\n",
    "\n",
    "```bash\n",
    "# Basic usage - single file\n",
    "python obs_pipeline.py -i input.nc -o output.bufr -t surface\n",
    "\n",
    "# Batch processing\n",
    "python obs_pipeline.py -i /data/observations/ -o /output/bufr/ -t surface --batch\n",
    "\n",
    "# With validation and verbose output\n",
    "python obs_pipeline.py -i input.nc -o output.bufr -t radiosonde --validate -v\n",
    "\n",
    "# Using custom configuration\n",
    "python obs_pipeline.py -i satellite.h5 -o satellite.bufr -t satellite -c config.json\n",
    "```\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `-i, --input`: Input file or directory path\n",
    "- `-o, --output`: Output file or directory path  \n",
    "- `-t, --type`: Observation type (`surface`, `radiosonde`, `satellite`)\n",
    "- `-c, --config`: Configuration file path\n",
    "- `--batch`: Enable batch processing mode\n",
    "- `--validate`: Validate output BUFR files\n",
    "- `-v, --verbose`: Enable verbose logging\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Sample Configuration File\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"centre\": 98,\n",
    "  \"subcenter\": 0,\n",
    "  \"quality_control\": true,\n",
    "  \"compression\": false,\n",
    "  \"output_format\": \"BUFR4\",\n",
    "  \"templates\": {\n",
    "    \"surface\": {\n",
    "      \"template_number\": 0,\n",
    "      \"descriptors\": [301150, 307080]\n",
    "    },\n",
    "    \"radiosonde\": {\n",
    "      \"template_number\": 2,\n",
    "      \"descriptors\": [309052]\n",
    "    },\n",
    "    \"satellite\": {\n",
    "      \"template_number\": 12,\n",
    "      \"descriptors\": [310026]\n",
    "    }\n",
    "  },\n",
    "  \"variable_mapping\": {\n",
    "    \"temperature_vars\": [\"temperature\", \"temp\", \"t2m\", \"air_temperature\"],\n",
    "    \"pressure_vars\": [\"pressure\", \"pres\", \"msl\", \"sea_level_pressure\"],\n",
    "    \"humidity_vars\": [\"humidity\", \"rh\", \"relative_humidity\", \"dewpoint\"],\n",
    "    \"wind_speed_vars\": [\"wind_speed\", \"wspd\", \"ws\", \"u10\", \"v10\"],\n",
    "    \"wind_direction_vars\": [\"wind_direction\", \"wdir\", \"wd\"],\n",
    "    \"precipitation_vars\": [\"precipitation\", \"precip\", \"rain\", \"tp\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Observation Types\n",
    "\n",
    "### 1. Surface Observations (SYNOP)\n",
    "\n",
    "**Input Format**: NetCDF with surface meteorological variables\n",
    "**BUFR Template**: 0 (Surface/land synoptic observations)\n",
    "**Key Variables**:\n",
    "- Temperature (2m air temperature)\n",
    "- Pressure (mean sea level pressure)\n",
    "- Humidity (relative humidity)\n",
    "- Wind speed/direction (10m wind)\n",
    "- Precipitation\n",
    "- Visibility\n",
    "\n",
    "**Example NetCDF Structure**:\n",
    "```\n",
    "dimensions:\n",
    "    station = 10 ;\n",
    "    time = 1 ;\n",
    "variables:\n",
    "    float latitude(station) ;\n",
    "    float longitude(station) ;\n",
    "    float temperature(station) ;\n",
    "    float pressure(station) ;\n",
    "    float humidity(station) ;\n",
    "    double time(time) ;\n",
    "```\n",
    "\n",
    "### 2. Radiosonde Observations\n",
    "\n",
    "**Input Format**: NetCDF with vertical profile data\n",
    "**BUFR Template**: 2 (Upper-air soundings)\n",
    "**Key Variables**:\n",
    "- Pressure levels\n",
    "- Temperature profile\n",
    "- Humidity profile\n",
    "- Wind speed/direction profile\n",
    "- Geopotential height\n",
    "\n",
    "**Example NetCDF Structure**:\n",
    "```\n",
    "dimensions:\n",
    "    level = 50 ;\n",
    "    time = 1 ;\n",
    "variables:\n",
    "    float pressure(level) ;\n",
    "    float temperature(level) ;\n",
    "    float humidity(level) ;\n",
    "    float wind_speed(level) ;\n",
    "    float latitude ;\n",
    "    float longitude ;\n",
    "```\n",
    "\n",
    "### 3. Satellite Observations\n",
    "\n",
    "**Input Format**: HDF5 with satellite retrievals\n",
    "**BUFR Template**: 12 (Satellite observations)\n",
    "**Key Variables**:\n",
    "- Brightness temperatures\n",
    "- Radiances\n",
    "- Retrieval products\n",
    "- Quality flags\n",
    "\n",
    "## BUFR Encoding Details\n",
    "\n",
    "### BUFR Templates Used\n",
    "\n",
    "1. **Surface Observations (Template 0)**\n",
    "   - Descriptors: 301150 (WMO station identification), 307080 (synoptic report)\n",
    "   - Category: 0 (Surface/land synoptic observations)\n",
    "\n",
    "2. **Radiosonde (Template 2)**  \n",
    "   - Descriptors: 309052 (Upper-air sounding)\n",
    "   - Category: 2 (Upper-air soundings)\n",
    "\n",
    "3. **Satellite (Template 12)**\n",
    "   - Descriptors: 310026 (Satellite radiance)\n",
    "   - Category: 12 (Satellite observations)\n",
    "\n",
    "### Key BUFR Parameters\n",
    "\n",
    "- **Centre**: 98 (ECMWF)\n",
    "- **Master Table Version**: 35 (WMO BUFR tables)\n",
    "- **Edition**: 4 (BUFR Edition 4)\n",
    "- **Compression**: Optional (configurable)\n",
    "\n",
    "## Quality Control\n",
    "\n",
    "The pipeline includes comprehensive quality control checks:\n",
    "\n",
    "### Surface Observations\n",
    "- Temperature range: -90°C to +60°C\n",
    "- Pressure range: 870 hPa to 1084.8 hPa\n",
    "- Humidity range: 0% to 100%\n",
    "- Wind speed: 0 m/s to 150 m/s\n",
    "\n",
    "### Radiosonde Profiles\n",
    "- Pressure monotonicity check\n",
    "- Temperature gradient validation\n",
    "- Humidity consistency\n",
    "- Wind shear limits\n",
    "\n",
    "### Quality Control Flags\n",
    "- `PASSED`: Observation passes all checks\n",
    "- `SUSPECT`: Observation questionable but retained\n",
    "- `REJECTED`: Observation fails critical checks\n",
    "\n",
    "## Error Handling and Logging\n",
    "\n",
    "The pipeline provides comprehensive error handling:\n",
    "\n",
    "```python\n",
    "# Example log output\n",
    "2025-06-07 12:00:00 - INFO - Processing surface_obs.nc -> surface_obs.bufr\n",
    "2025-06-07 12:00:00 - INFO - Extracted 150 surface observations\n",
    "2025-06-07 12:00:00 - WARNING - Missing wind direction for station 12345\n",
    "2025-06-07 12:00:00 - INFO - Encoded 150 surface observations to surface_obs.bufr\n",
    "2025-06-07 12:00:00 - INFO - BUFR validation: 150 messages found\n",
    "```\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "### Memory Usage\n",
    "- Large NetCDF files are processed in chunks\n",
    "- Streaming processing for satellite swath data\n",
    "- Configurable buffer sizes\n",
    "\n",
    "### Processing Speed\n",
    "- Typical throughput: 1000 observations/second\n",
    "- Parallel processing support for batch mode\n",
    "- Optimized BUFR encoding with ecCodes\n",
    "\n",
    "## Testing and Validation\n",
    "\n",
    "### Create Test Data\n",
    "\n",
    "```python\n",
    "# Generate test surface observations\n",
    "python obs_pipeline.py --create-test-surface\n",
    "\n",
    "# Generate test radiosonde data  \n",
    "python obs_pipeline.py --create-test-radiosonde\n",
    "\n",
    "# Create sample configuration\n",
    "python obs_pipeline.py --create-sample-config\n",
    "```\n",
    "\n",
    "### Validation Tools\n",
    "\n",
    "```python\n",
    "# Validate BUFR output\n",
    "from obs_pipeline import ObservationProcessor\n",
    "processor = ObservationProcessor()\n",
    "is_valid = processor.validate_bufr_output('output.bufr')\n",
    "```\n",
    "\n",
    "## Integration with ECMWF Systems\n",
    "\n",
    "### IFS Integration\n",
    "The pipeline can be integrated with IFS (Integrated Forecasting System):\n",
    "\n",
    "```bash\n",
    "# Process observations for IFS\n",
    "python obs_pipeline.py -i /mars/obs/synop/ -o /mars/bufr/synop/ -t surface --batch\n",
    "```\n",
    "\n",
    "### Mars Integration\n",
    "Output BUFR files are compatible with MARS archival:\n",
    "\n",
    "```python\n",
    "# MARS archival example\n",
    "import subprocess\n",
    "subprocess.run(['mars', 'put', 'bufr_file.bufr'])\n",
    "```\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **ecCodes Installation Problems**\n",
    "   ```bash\n",
    "   # Check ecCodes installation\n",
    "   python -c \"import eccodes; print('ecCodes version:', eccodes.codes_get_api_version())\"\n",
    "   ```\n",
    "\n",
    "2. **NetCDF Variable Mapping**\n",
    "   - Check variable names in NetCDF file: `ncdump -h file.nc`\n",
    "   - Update variable mapping in configuration file\n",
    "\n",
    "3. **BUFR Encoding Errors**\n",
    "   - Verify BUFR template descriptors\n",
    "   - Check for missing mandatory variables\n",
    "   - Validate coordinate ranges\n",
    "\n",
    "4. **Memory Issues with Large Files**\n",
    "   - Enable chunked processing\n",
    "   - Reduce buffer sizes in configuration\n",
    "   - Use streaming mode for satellite data\n",
    "\n",
    "### Debug Mode\n",
    "\n",
    "```bash\n",
    "# Enable debug logging\n",
    "python obs_pipeline.py -i input.nc -o output.bufr -v --debug\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "1. **WMO Manual on Codes (WMO-No. 306)**\n",
    "   - Volume I.2: Binary Universal Form for the Representation of meteorological data (BUFR)\n",
    "\n",
    "2. **ECMWF ecCodes Documentation**\n",
    "   - https://confluence.ecmwf.int/display/ECC/ecCodes+Home\n",
    "\n",
    "3. **WMO BUFR Templates**\n",
    "   - https://www.wmo.int/pages/prog/www/WMOCodes/WMO306_vI2/LatestVERSION/WMO306_vI2_BUFRCREX_TableB_en.pdf\n",
    "\n",
    "4. **CF Conventions for NetCDF**\n",
    "   - http://cfconventions.org/\n",
    "\n",
    "5. **ECMWF Data Standards**\n",
    "   - Internal ECMWF documentation on observation processing standards\n",
    "\n",
    "## Version History\n",
    "\n",
    "- **v1.0.0** (June 2025): Initial release with surface, radiosonde, and satellite support\n",
    "- **v0.9.0** (May 2025): Beta release with basic functionality\n",
    "- **v0.1.0** (April 2025): Development version\n",
    "\n",
    "## Support\n",
    "\n",
    "For technical support and questions:\n",
    "- Internal ECMWF: Contact the Observation Processing Team\n",
    "- External users: Refer to ecCodes community forums\n",
    "\n",
    "## License\n",
    "\n",
    "This software is developed for ECMWF internal use and follows ECMWF software licensing terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b791ed6-1758-4dd0-9243-e128e1a725d1",
   "metadata": {},
   "source": [
    "# High-Level Documentation for an Early-Career Scientist\n",
    "## ECMWF Observation Processing Workflow: From Raw Data to BUFR\n",
    "1. Introduction: The \"What\" and the \"Why\"\n",
    "\n",
    "Welcome to the ECMWF observation processing workflow! At its core, this is a standard software pipeline with a simple but critical purpose: to take meteorological data from a scientific format (NetCDF) and convert it into a highly structured, operational format called BUFR.\n",
    "\n",
    "Why is this important?\n",
    "\n",
    "    Standardization: The World Meteorological Organization (WMO) has designated BUFR (Binary Universal Form for the Representation of meteorological data) as the standard format for exchanging observational data. When we receive data from weather stations, satellites, or buoys, it eventually gets converted to BUFR.\n",
    "\n",
    "    Operational Use: Weather prediction models, like the one at ECMWF, don't just use gridded model data; they \"assimilate\" real-world observations to correct the model's forecast. This assimilation system is designed to read BUFR files.\n",
    "\n",
    "    Efficiency: BUFR is a compact, binary format that is much more efficient for storage and transmission than text-based formats.\n",
    "\n",
    "This workflow contains two main Python scripts:\n",
    "\n",
    "    ecmwf_data_retrieval.py: Fetches or creates realistic source data.\n",
    "\n",
    "    obs_pipeline.py: Performs the conversion from NetCDF to BUFR.\n",
    "\n",
    "2. Component 1: The Data Source (ecmwf_data_retrieval.py)\n",
    "\n",
    "Purpose: To provide a reliable and consistent source of meteorological data to test our pipeline.\n",
    "\n",
    "Data Source: ERA5 Reanalysis\n",
    "\n",
    "Initially, we tried to get data from individual observation networks, but these can be difficult to access programmatically. Instead, we have adopted a more robust approach used widely in atmospheric science: we use data from ERA5.\n",
    "\n",
    "ERA5 is a \"reanalysis\" dataset. This means ECMWF has run a modern weather model for the entire globe, going back decades, and has assimilated all available historical observations. The result is a physically consistent, high-resolution gridded dataset that is considered the \"best guess\" of the atmosphere's state at any given time.\n",
    "\n",
    "For our purposes, we treat each grid point in the ERA5 dataset as a \"virtual weather station\". This gives us a stable and predictable source of realistic data.\n",
    "\n",
    "How it Works:\n",
    "The script uses the Copernicus Climate Data Store (CDS) API to:\n",
    "\n",
    "    Request a small slice of the global ERA5 grid (e.g., surface temperature over Central Europe for a specific day).\n",
    "\n",
    "    The CDS packages this data and sends it back as a NetCDF file. NetCDF is a very common format in science for storing multidimensional array data (e.g., [time, latitude, longitude]).\n",
    "\n",
    "    For offline testing, if you don't use the --real flag, the script will generate a synthetic NetCDF file that has the exact same structure as a real ERA5 file.\n",
    "\n",
    "3. Component 2: The Processing Pipeline (obs_pipeline.py)\n",
    "\n",
    "Purpose: To read the gridded NetCDF data and convert each grid point into an individual BUFR message.\n",
    "Step A: Reading the Data (The NetCDFReader)\n",
    "\n",
    "The pipeline first needs to understand the incoming NetCDF file. The NetCDFReader does the following:\n",
    "\n",
    "    It opens the NetCDF file and receives the base date of the data from the command line (e.g., 2025-06-01).\n",
    "\n",
    "    It reads the coordinate arrays (latitude, longitude, time, level).\n",
    "\n",
    "    It then begins a large loop. For every time step and every latitude/longitude coordinate, it \"flattens\" the grid, extracting the data values (like temperature and pressure) for that single point.\n",
    "\n",
    "    Each of these grid points is now treated as an individual observation report, ready to be encoded.\n",
    "\n",
    "Step B: Encoding the Data (The BUFREncoder)\n",
    "\n",
    "This is the core of the workflow. The encoder takes a single observation report (from one grid point at one time) and builds a BUFR message.\n",
    "\n",
    "Here's how it works, following the strict requirements of the eccodes library:\n",
    "\n",
    "    Create a Container: An empty, generic BUFR message is created.\n",
    "\n",
    "    Define the Structure: We load a standard WMO template into the message. This is done by setting the unexpandedDescriptors, which is just a sequence of numeric codes. For example, the sequence `` tells the message: \"You are a SYNOP (surface) report and you must contain fields for location, time, temperature, pressure, and wind.\"\n",
    "\n",
    "    Build the Data Section: We call the crucial pack command. This tells eccodes to read the descriptor template and build the actual data section in memory, creating a \"slot\" for every piece of data.\n",
    "\n",
    "    Fill in the Blanks: Now that the slots exist, the script can safely set the value for each key (e.g., ec.codes_set(bufr, \"airTemperature\", 285.3)).\n",
    "\n",
    "    Write to File: The final, complete binary message is appended to our output file.\n",
    "\n",
    "This process repeats for every single grid point, resulting in a BUFR file containing thousands of individual messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4e509-5f05-434c-8487-08815e5efd22",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0381456-b23a-4a69-bfa1-7998473c7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting obs_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile obs_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ECMWF Observation Processing Pipeline\n",
    "Converts gridded ERA5 NetCDF data into a series of point-based BUFR messages.\n",
    "This version contains the definitive fix for the eccodes \"Key/value not found\"\n",
    "error by ensuring the correct BUFR creation sequence.\n",
    "\n",
    "Author: ECMWF Senior Software Engineer\n",
    "\"\"\"\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import eccodes as ec\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NetCDFReader:\n",
    "    \"\"\"Reads gridded ERA5 NetCDF files and extracts data as a list of point observations.\"\"\"\n",
    "    def __init__(self, filepath: str, base_date: datetime):\n",
    "        self.filepath = Path(filepath)\n",
    "        self.base_date = base_date\n",
    "        if not self.filepath.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {self.filepath}\")\n",
    "\n",
    "    def _get_var(self, ds, names: List[str]):\n",
    "        for name in names:\n",
    "            if name in ds.variables: return ds.variables[name]\n",
    "        raise KeyError(f\"Could not find any of required variables: {names} in {self.filepath}. This may be due to an error during data generation.\")\n",
    "\n",
    "    def extract_surface_observations(self) -> List[Dict]:\n",
    "        \"\"\"Extracts each grid point as a separate surface observation.\"\"\"\n",
    "        observations = []\n",
    "        with nc.Dataset(self.filepath, 'r') as ds:\n",
    "            lats, lons = self._get_var(ds, ['latitude'])[:], self._get_var(ds, ['longitude'])[:]\n",
    "            times_hours = self._get_var(ds, ['time'])[:]\n",
    "            t2m, msl, u10, v10 = (\n",
    "                self._get_var(ds, ['t2m']), self._get_var(ds, ['msl']),\n",
    "                self._get_var(ds, ['u10']), self._get_var(ds, ['v10'])\n",
    "            )\n",
    "            \n",
    "            for t_idx, hour in enumerate(times_hours):\n",
    "                obs_time = self.base_date + timedelta(hours=int(hour))\n",
    "                for lat_idx, lat in enumerate(lats):\n",
    "                    for lon_idx, lon in enumerate(lons):\n",
    "                        observations.append({\n",
    "                            'latitude': float(lat), 'longitude': float(lon),\n",
    "                            'station_id': 99000 + (lat_idx * len(lons) + lon_idx) % 999,\n",
    "                            'time': obs_time,\n",
    "                            'temperature': float(t2m[t_idx, lat_idx, lon_idx]),\n",
    "                            'pressure': float(msl[t_idx, lat_idx, lon_idx]),\n",
    "                            'u_wind': float(u10[t_idx, lat_idx, lon_idx]),\n",
    "                            'v_wind': float(v10[t_idx, lat_idx, lon_idx]),\n",
    "                        })\n",
    "        logger.info(f\"Extracted {len(observations)} point observations from {self.filepath.name}\")\n",
    "        return observations\n",
    "\n",
    "    def extract_upper_air_observations(self) -> List[Dict]:\n",
    "        \"\"\"Extracts each grid point as a separate radiosonde profile.\"\"\"\n",
    "        observations = []\n",
    "        with nc.Dataset(self.filepath, 'r') as ds:\n",
    "            lats, lons, times_hours, levels = (\n",
    "                self._get_var(ds, ['latitude'])[:], self._get_var(ds, ['longitude'])[:],\n",
    "                self._get_var(ds, ['time'])[:], self._get_var(ds, ['level'])[:]\n",
    "            )\n",
    "            temp, rh, u, v = (\n",
    "                self._get_var(ds, ['t']), self._get_var(ds, ['r']),\n",
    "                self._get_var(ds, ['u']), self._get_var(ds, ['v'])\n",
    "            )\n",
    "            for t_idx, hour in enumerate(times_hours):\n",
    "                obs_time = self.base_date + timedelta(hours=int(hour))\n",
    "                for lat_idx, lat in enumerate(lats):\n",
    "                    for lon_idx, lon in enumerate(lons):\n",
    "                        profile = [{\n",
    "                            'pressure': float(level * 100),\n",
    "                            'temperature': float(temp[t_idx, l_idx, lat_idx, lon_idx]),\n",
    "                            'humidity': float(rh[t_idx, l_idx, lat_idx, lon_idx]),\n",
    "                            'u_wind': float(u[t_idx, l_idx, lat_idx, lon_idx]),\n",
    "                            'v_wind': float(v[t_idx, l_idx, lat_idx, lon_idx]),\n",
    "                        } for l_idx, level in enumerate(levels)]\n",
    "                        observations.append({\n",
    "                            'latitude': float(lat), 'longitude': float(lon),\n",
    "                            'time': obs_time, 'station_id': 99000 + (lat_idx*len(lons)+lon_idx)%999,\n",
    "                            'profile': profile,\n",
    "                        })\n",
    "        logger.info(f\"Extracted {len(observations)} profiles from {self.filepath.name}\")\n",
    "        return observations\n",
    "\n",
    "class BUFREncoder:\n",
    "    \"\"\"\n",
    "    Encodes observation data into WMO BUFR format using the correct, robust\n",
    "    sequence of eccodes operations.\n",
    "    \"\"\"\n",
    "    def encode(self, observations: List[Dict], output_file: str, obs_type: str):\n",
    "        encoder_map = {'surface': self._encode_surface, 'upper_air': self._encode_upper_air}\n",
    "        with open(output_file, 'wb') as f:\n",
    "            count = 0\n",
    "            for obs in observations:\n",
    "                bufr_msg = None\n",
    "                try:\n",
    "                    bufr_msg = encoder_map[obs_type](obs)\n",
    "                    if bufr_msg:\n",
    "                        ec.codes_write(bufr_msg, f)\n",
    "                        count += 1\n",
    "                except Exception as e:\n",
    "                    # Provide a more detailed error log entry\n",
    "                    logger.error(f\"Failed to encode obs for station {obs.get('station_id', 'N/A')} at time {obs.get('time', 'N/A')}: {e}\", exc_info=False)\n",
    "                finally:\n",
    "                    if bufr_msg: ec.codes_release(bufr_msg)\n",
    "        logger.info(f\"Successfully encoded {count} BUFR messages to {output_file}\")\n",
    "\n",
    "    def _encode_surface(self, obs: Dict) -> int:\n",
    "        \"\"\"Encodes a single surface observation using the correct, robust sequence.\"\"\"\n",
    "        # Try the standard WMO surface observation template first\n",
    "        try:\n",
    "            return self._encode_surface_synop(obs)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Standard SYNOP encoding failed: {e}\")\n",
    "            # Fallback to simple template\n",
    "            return self._encode_surface_simple(obs)\n",
    "\n",
    "    def _encode_surface_synop(self, obs: Dict) -> int:\n",
    "        \"\"\"Encode using standard WMO SYNOP template.\"\"\"\n",
    "        bufr = ec.codes_bufr_new_from_samples(\"BUFR4\")\n",
    "        \n",
    "        # Set basic header\n",
    "        ec.codes_set(bufr, \"masterTableNumber\", 0)\n",
    "        ec.codes_set(bufr, \"bufrHeaderCentre\", 98)\n",
    "        ec.codes_set(bufr, \"bufrHeaderSubCentre\", 0)\n",
    "        ec.codes_set(bufr, \"updateSequenceNumber\", 0)\n",
    "        ec.codes_set(bufr, \"dataCategory\", 0)\n",
    "        ec.codes_set(bufr, \"internationalDataSubCategory\", 0)\n",
    "        ec.codes_set(bufr, \"masterTablesVersionNumber\", 28)\n",
    "        ec.codes_set(bufr, \"localTablesVersionNumber\", 0)\n",
    "        \n",
    "        # Time\n",
    "        obs_time = obs['time']\n",
    "        ec.codes_set(bufr, \"typicalYear\", obs_time.year)\n",
    "        ec.codes_set(bufr, \"typicalMonth\", obs_time.month)\n",
    "        ec.codes_set(bufr, \"typicalDay\", obs_time.day)\n",
    "        ec.codes_set(bufr, \"typicalHour\", obs_time.hour)\n",
    "        ec.codes_set(bufr, \"typicalMinute\", obs_time.minute)\n",
    "        \n",
    "        # Subsets\n",
    "        ec.codes_set(bufr, \"numberOfSubsets\", 1)\n",
    "        ec.codes_set(bufr, \"observedData\", 1)\n",
    "        ec.codes_set(bufr, \"compressedData\", 0)\n",
    "        \n",
    "        # Use standard SYNOP template 307080 (Land station surface)\n",
    "        ec.codes_set_array(bufr, \"unexpandedDescriptors\", [307080])\n",
    "        \n",
    "        # Pack to create structure\n",
    "        ec.codes_set(bufr, \"pack\", 1)\n",
    "        \n",
    "        # Set data values using available keys\n",
    "        self._set_synop_data(bufr, obs, obs_time)\n",
    "        \n",
    "        return bufr\n",
    "    \n",
    "    def _set_synop_data(self, bufr, obs: Dict, obs_time: datetime):\n",
    "        \"\"\"Set data values for SYNOP template, handling missing keys gracefully.\"\"\"\n",
    "        # Helper function to safely set values\n",
    "        def safe_set(key, value, subset_num=1):\n",
    "            try:\n",
    "                full_key = f\"#{subset_num}#{key}\" if subset_num else key\n",
    "                ec.codes_set(bufr, full_key, value)\n",
    "                return True\n",
    "            except:\n",
    "                try:\n",
    "                    # Try without subset notation\n",
    "                    ec.codes_set(bufr, key, value)\n",
    "                    return True\n",
    "                except:\n",
    "                    return False\n",
    "        \n",
    "        # Time data\n",
    "        safe_set(\"year\", obs_time.year)\n",
    "        safe_set(\"month\", obs_time.month) \n",
    "        safe_set(\"day\", obs_time.day)\n",
    "        safe_set(\"hour\", obs_time.hour)\n",
    "        safe_set(\"minute\", obs_time.minute)\n",
    "        \n",
    "        # Station identification\n",
    "        station_id = obs['station_id']\n",
    "        safe_set(\"blockNumber\", station_id // 1000)\n",
    "        safe_set(\"stationNumber\", station_id % 1000)\n",
    "        safe_set(\"stationOrSiteName\", f\"ERA5_{station_id}\")\n",
    "        \n",
    "        # Location\n",
    "        safe_set(\"latitude\", obs['latitude'])\n",
    "        safe_set(\"longitude\", obs['longitude'])\n",
    "        safe_set(\"heightOfStation\", 0.0)  # Default to sea level\n",
    "        \n",
    "        # Meteorological parameters\n",
    "        if not np.isnan(obs['temperature']):\n",
    "            safe_set(\"airTemperature\", obs['temperature'])\n",
    "            safe_set(\"temperature\", obs['temperature'])  # Alternative key\n",
    "            \n",
    "        if not np.isnan(obs['pressure']):\n",
    "            safe_set(\"pressureReducedToMeanSeaLevel\", obs['pressure'])\n",
    "            safe_set(\"pressure\", obs['pressure'])  # Alternative key\n",
    "            \n",
    "        # Wind data\n",
    "        if not np.isnan(obs['u_wind']) and not np.isnan(obs['v_wind']):\n",
    "            wind_speed = np.sqrt(obs['u_wind']**2 + obs['v_wind']**2)\n",
    "            wind_dir = (np.arctan2(-obs['u_wind'], -obs['v_wind']) * 180.0 / np.pi) % 360\n",
    "            \n",
    "            safe_set(\"windSpeed\", wind_speed)\n",
    "            safe_set(\"windDirection\", wind_dir)\n",
    "            safe_set(\"uComponentOfWind\", obs['u_wind'])\n",
    "            safe_set(\"vComponentOfWind\", obs['v_wind'])\n",
    "\n",
    "    def _encode_surface_simple(self, obs: Dict) -> int:\n",
    "        \"\"\"Fallback method with minimal BUFR structure.\"\"\"\n",
    "        bufr = ec.codes_bufr_new_from_samples(\"BUFR4_local\")\n",
    "        \n",
    "        # Minimal header setup\n",
    "        ec.codes_set(bufr, \"bufrHeaderCentre\", 98)\n",
    "        ec.codes_set(bufr, \"dataCategory\", 0)\n",
    "        ec.codes_set(bufr, \"numberOfSubsets\", 1)\n",
    "        ec.codes_set(bufr, \"compressedData\", 0)\n",
    "        ec.codes_set(bufr, \"observedData\", 1)\n",
    "        \n",
    "        # Use basic identification and location template\n",
    "        ec.codes_set_array(bufr, \"unexpandedDescriptors\", [301011, 301012, 301021])\n",
    "        ec.codes_set(bufr, \"pack\", 1)\n",
    "        \n",
    "        # Set minimal essential data\n",
    "        obs_time = obs['time']\n",
    "        try:\n",
    "            ec.codes_set(bufr, \"year\", obs_time.year)\n",
    "            ec.codes_set(bufr, \"month\", obs_time.month)\n",
    "            ec.codes_set(bufr, \"day\", obs_time.day)\n",
    "            ec.codes_set(bufr, \"hour\", obs_time.hour)\n",
    "            ec.codes_set(bufr, \"minute\", obs_time.minute)\n",
    "            ec.codes_set(bufr, \"latitude\", obs['latitude'])\n",
    "            ec.codes_set(bufr, \"longitude\", obs['longitude'])\n",
    "            \n",
    "            # Try to set station info\n",
    "            station_id = obs['station_id']\n",
    "            try:\n",
    "                ec.codes_set(bufr, \"blockNumber\", station_id // 1000)\n",
    "                ec.codes_set(bufr, \"stationNumber\", station_id % 1000)\n",
    "            except:\n",
    "                pass  # Skip if not available\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Minimal encoding also had issues: {e}\")\n",
    "        \n",
    "        return bufr\n",
    "\n",
    "    def _encode_upper_air(self, obs: Dict) -> int:\n",
    "        \"\"\"Encodes a single upper-air profile using the correct, robust sequence.\"\"\"\n",
    "        bufr = ec.codes_bufr_new_from_samples(\"BUFR4\")\n",
    "        \n",
    "        # 1. Set essential header information\n",
    "        ec.codes_set(bufr, \"edition\", 4)\n",
    "        ec.codes_set(bufr, \"masterTableNumber\", 0)\n",
    "        ec.codes_set(bufr, \"bufrHeaderCentre\", 98)\n",
    "        ec.codes_set(bufr, \"bufrHeaderSubCentre\", 0)\n",
    "        ec.codes_set(bufr, \"updateSequenceNumber\", 0)\n",
    "        ec.codes_set(bufr, \"dataCategory\", 2)  # Upper-air category\n",
    "        ec.codes_set(bufr, \"internationalDataSubCategory\", 0)\n",
    "        ec.codes_set(bufr, \"masterTablesVersionNumber\", 37)\n",
    "        ec.codes_set(bufr, \"localTablesVersionNumber\", 0)\n",
    "        \n",
    "        # 2. Set time information\n",
    "        obs_time = obs['time']\n",
    "        ec.codes_set(bufr, \"typicalYear\", obs_time.year)\n",
    "        ec.codes_set(bufr, \"typicalMonth\", obs_time.month)\n",
    "        ec.codes_set(bufr, \"typicalDay\", obs_time.day)\n",
    "        ec.codes_set(bufr, \"typicalHour\", obs_time.hour)\n",
    "        ec.codes_set(bufr, \"typicalMinute\", obs_time.minute)\n",
    "        ec.codes_set(bufr, \"typicalSecond\", obs_time.second)\n",
    "        \n",
    "        # 3. Set subset information\n",
    "        ec.codes_set(bufr, \"numberOfSubsets\", 1)\n",
    "        ec.codes_set(bufr, \"observedData\", 1)\n",
    "        ec.codes_set(bufr, \"compressedData\", 0)\n",
    "        \n",
    "        # 4. Set replication factor for profile levels\n",
    "        profile = obs['profile']\n",
    "        ec.codes_set(bufr, \"delayedDescriptorReplicationFactor\", len(profile))\n",
    "        \n",
    "        # 5. Define structure for radiosonde\n",
    "        ec.codes_set_array(bufr, \"unexpandedDescriptors\", [301011, 301012, 301021, 101000 + len(profile), 31001, 7004, 12101])\n",
    "        \n",
    "        # 6. Pack the template\n",
    "        ec.codes_set(bufr, \"pack\", 1)\n",
    "        \n",
    "        # 7. Set header data\n",
    "        try:\n",
    "            ec.codes_set(bufr, \"year\", obs_time.year)\n",
    "            ec.codes_set(bufr, \"month\", obs_time.month)\n",
    "            ec.codes_set(bufr, \"day\", obs_time.day)\n",
    "            ec.codes_set(bufr, \"hour\", obs_time.hour)\n",
    "            ec.codes_set(bufr, \"latitude\", obs['latitude'])\n",
    "            ec.codes_set(bufr, \"longitude\", obs['longitude'])\n",
    "            \n",
    "            # Set profile data as arrays\n",
    "            pressures = [p['pressure'] for p in profile]\n",
    "            temperatures = [p['temperature'] for p in profile]\n",
    "            \n",
    "            ec.codes_set_array(bufr, \"pressure\", pressures)\n",
    "            ec.codes_set_array(bufr, \"airTemperature\", temperatures)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not set profile data: {e}\")\n",
    "            # Return a minimal upper-air message\n",
    "            return self._encode_upper_air_simple(obs)\n",
    "        \n",
    "        return bufr\n",
    "\n",
    "    def _encode_upper_air_simple(self, obs: Dict) -> int:\n",
    "        \"\"\"Fallback method for upper-air with minimal structure.\"\"\"\n",
    "        bufr = ec.codes_bufr_new_from_samples(\"BUFR4_local\")\n",
    "        \n",
    "        ec.codes_set(bufr, \"bufrHeaderCentre\", 98)\n",
    "        ec.codes_set(bufr, \"dataCategory\", 2)\n",
    "        ec.codes_set(bufr, \"numberOfSubsets\", 1)\n",
    "        ec.codes_set(bufr, \"compressedData\", 0)\n",
    "        \n",
    "        # Basic structure\n",
    "        ec.codes_set_array(bufr, \"unexpandedDescriptors\", [301011, 301012])\n",
    "        ec.codes_set(bufr, \"pack\", 1)\n",
    "        \n",
    "        # Essential data only\n",
    "        obs_time = obs['time']\n",
    "        ec.codes_set(bufr, \"year\", obs_time.year)\n",
    "        ec.codes_set(bufr, \"month\", obs_time.month)\n",
    "        ec.codes_set(bufr, \"day\", obs_time.day)\n",
    "        ec.codes_set(bufr, \"hour\", obs_time.hour)\n",
    "        ec.codes_set(bufr, \"latitude\", obs['latitude'])\n",
    "        ec.codes_set(bufr, \"longitude\", obs['longitude'])\n",
    "        \n",
    "        return bufr\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"ECMWF ERA5 to BUFR Pipeline\")\n",
    "    parser.add_argument('-i', '--input', required=True, help='Input NetCDF file path')\n",
    "    parser.add_argument('-o', '--output', required=True, help='Output BUFR file path')\n",
    "    parser.add_argument('-t', '--type', choices=['surface', 'upper_air'], required=True)\n",
    "    parser.add_argument('-d', '--date', required=True, help='Base date of the data in YYYY-MM-DD format')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    try:\n",
    "        base_date = datetime.strptime(args.date, '%Y-%m-%d')\n",
    "        reader = NetCDFReader(args.input, base_date)\n",
    "        \n",
    "        if args.type == 'surface':\n",
    "            observations = reader.extract_surface_observations()\n",
    "        else:\n",
    "            observations = reader.extract_upper_air_observations()\n",
    "\n",
    "        if observations:\n",
    "            encoder = BUFREncoder()\n",
    "            encoder.encode(observations, args.output, args.type)\n",
    "            print(f\"Processing complete. Output: {args.output}\")\n",
    "        else:\n",
    "            logger.warning(\"No valid observations were extracted. No output file was created.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"A critical error occurred: {e}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d00ff-a541-4bca-a91d-5ea6a186eaae",
   "metadata": {},
   "source": [
    "# Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04ecc4c-2532-455c-8609-6571ca95bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ecmwf_data_retrieval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ecmwf_data_retrieval.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ECMWF Data Retrieval Functions for Observation Pipeline Testing\n",
    "\n",
    "This module uses stable, public ERA5 reanalysis datasets from the CDS.\n",
    "This version contains the definitive fix for all identified bugs.\n",
    "\n",
    "Author: ECMWF Senior Software Engineer\n",
    "\"\"\"\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    from cdsapi import Client as CDSClient\n",
    "    ECMWF_API_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ECMWF_API_AVAILABLE = False\n",
    "    logging.warning(\"cdsapi library not found. Real data retrieval is disabled.\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class ECMWFTestDataGenerator:\n",
    "    \"\"\"\n",
    "    Generates and retrieves test data using the stable ERA5 reanalysis datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_file: Optional[str] = None):\n",
    "        self.config = {\n",
    "            \"area\": [55, 5, 50, 15],  # North, West, South, East (Central Europe)\n",
    "            \"grid\": [0.25, 0.25],\n",
    "        }\n",
    "        self.cds_client = None\n",
    "        if ECMWF_API_AVAILABLE:\n",
    "            try:\n",
    "                self.cds_client = CDSClient()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to initialize CDS client: {e}. Real data retrieval disabled.\")\n",
    "                self.cds_client = None\n",
    "\n",
    "    def retrieve_surface_data(self, output_path: str, date: str, use_real_data: bool = False) -> str:\n",
    "        \"\"\"Retrieves surface-level data. Uses ERA5 single levels for real data. [1, 2]\"\"\"\n",
    "        logger.info(f\"Preparing surface data for {date}\")\n",
    "        if use_real_data and self.cds_client:\n",
    "            return self._retrieve_real_surface_data(output_path, date)\n",
    "        else:\n",
    "            if use_real_data: logger.warning(\"Real data requested but API client not available. Falling back to synthetic.\")\n",
    "            return self._generate_synthetic_surface_data(output_path, date)\n",
    "\n",
    "    def retrieve_upper_air_data(self, output_path: str, date: str, use_real_data: bool = False) -> str:\n",
    "        \"\"\"Retrieves upper-air data. Uses ERA5 pressure levels for real data. [3]\"\"\"\n",
    "        logger.info(f\"Preparing upper-air data for {date}\")\n",
    "        if use_real_data and self.cds_client:\n",
    "            return self._retrieve_real_upper_air_data(output_path, date)\n",
    "        else:\n",
    "            if use_real_data: logger.warning(\"Real data requested but API client not available. Falling back to synthetic.\")\n",
    "            return self._generate_synthetic_upper_air_data(output_path, date)\n",
    "\n",
    "    def _retrieve_real_surface_data(self, output_path: str, date: str) -> str:\n",
    "        \"\"\"Retrieve real surface data from the ERA5 single levels dataset. [1, 2]\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Retrieving real surface data from CDS: 'reanalysis-era5-single-levels'\")\n",
    "            self.cds_client.retrieve(\n",
    "                'reanalysis-era5-single-levels',\n",
    "                {\n",
    "                    'product_type': 'reanalysis',\n",
    "                    'variable': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'mean_sea_level_pressure'],\n",
    "                    'year': date[:4], 'month': date[5:7], 'day': date[8:10],\n",
    "                    'time': ['00:00', '06:00', '12:00', '18:00'],\n",
    "                    'area': self.config['area'], 'grid': self.config['grid'], 'format': 'netcdf',\n",
    "                },\n",
    "                output_path)\n",
    "            logger.info(f\"Successfully retrieved ERA5 surface data to {output_path}\")\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to retrieve ERA5 surface data: {e}. Falling back to synthetic generation.\")\n",
    "            return self._generate_synthetic_surface_data(output_path, date)\n",
    "\n",
    "    def _retrieve_real_upper_air_data(self, output_path: str, date: str) -> str:\n",
    "        \"\"\"Retrieve real upper-air data from the ERA5 pressure levels dataset. [3]\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Retrieving real upper-air data from CDS: 'reanalysis-era5-pressure-levels'\")\n",
    "            self.cds_client.retrieve(\n",
    "                'reanalysis-era5-pressure-levels',\n",
    "                {\n",
    "                    'product_type': 'reanalysis',\n",
    "                    'variable': ['temperature', 'relative_humidity', 'u_component_of_wind', 'v_component_of_wind'],\n",
    "                    'pressure_level': ['1000', '850', '700', '500', '300'],\n",
    "                    'year': date[:4], 'month': date[5:7], 'day': date[8:10],\n",
    "                    'time': ['00:00', '12:00'],\n",
    "                    'area': self.config['area'], 'grid': self.config['grid'], 'format': 'netcdf',\n",
    "                },\n",
    "                output_path)\n",
    "            logger.info(f\"Successfully retrieved ERA5 pressure level data to {output_path}\")\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to retrieve ERA5 pressure level data: {e}. Falling back to synthetic generation.\")\n",
    "            return self._generate_synthetic_upper_air_data(output_path, date)\n",
    "\n",
    "    def _generate_synthetic_surface_data(self, output_path: str, date: str) -> str:\n",
    "        logger.info(f\"Generating synthetic surface data: {output_path}\")\n",
    "        with nc.Dataset(output_path, 'w', format='NETCDF4') as ds:\n",
    "            lats, lons = np.mgrid[self.config['area'][0]:self.config['area'][2]:-self.config['grid'][0], self.config['area'][1]:self.config['area'][3]:self.config['grid'][1]]\n",
    "            times = [0, 6, 12, 18]\n",
    "            ds.createDimension('latitude', lats.shape[0]); ds.createVariable('latitude', 'f4', ('latitude',))[:] = lats[:,0]\n",
    "            ds.createDimension('longitude', lons.shape[1]); ds.createVariable('longitude', 'f4', ('longitude',))[:] = lons[0,:]\n",
    "            ds.createDimension('time', len(times)); ds.createVariable('time', 'i4', ('time',))[:] = times\n",
    "            var_shape = ('time', 'latitude', 'longitude')\n",
    "            ds.createVariable('t2m', 'f4', var_shape)[:] = 285 + np.random.randn(len(times), lats.shape[0], lons.shape[1]) * 5\n",
    "            ds.createVariable('msl', 'f4', var_shape)[:] = 101325 + np.random.randn(len(times), lats.shape[0], lons.shape[1]) * 500\n",
    "            ds.createVariable('u10', 'f4', var_shape)[:] = 5 + np.random.randn(len(times), lats.shape[0], lons.shape[1]) * 3\n",
    "            ds.createVariable('v10', 'f4', var_shape)[:] = 2 + np.random.randn(len(times), lats.shape[0], lons.shape[1]) * 3\n",
    "        return output_path\n",
    "\n",
    "    def _generate_synthetic_upper_air_data(self, output_path: str, date: str) -> str:\n",
    "        logger.info(f\"Generating synthetic upper-air data: {output_path}\")\n",
    "        with nc.Dataset(output_path, 'w', format='NETCDF4') as ds:\n",
    "            lats, lons = np.mgrid[self.config['area'][0]:self.config['area'][2]:-self.config['grid'][0], self.config['area'][1]:self.config['area'][3]:self.config['grid'][1]]\n",
    "            times = [0, 12]; levels = [1000, 850, 700, 500, 300]\n",
    "            ds.createDimension('latitude', lats.shape[0]); ds.createVariable('latitude', 'f4', ('latitude',))[:] = lats[:,0]\n",
    "            ds.createDimension('longitude', lons.shape[1]); ds.createVariable('longitude', 'f4', ('longitude',))[:] = lons[0,:]\n",
    "            ds.createDimension('time', len(times)); ds.createVariable('time', 'i4', ('time',))[:] = times\n",
    "            ds.createDimension('level', len(levels)); ds.createVariable('level', 'i4', ('level',))[:] = levels\n",
    "            var_shape = ('time', 'level', 'latitude', 'longitude')\n",
    "            ds.createVariable('t', 'f4', var_shape)[:] = 270 + np.random.randn(len(times), len(levels), lats.shape[0], lons.shape[1]) * 10\n",
    "            ds.createVariable('r', 'f4', var_shape)[:] = 50 + np.random.randn(len(times), len(levels), lats.shape[0], lons.shape[1]) * 20\n",
    "            ds.createVariable('u', 'f4', var_shape)[:] = 5 + np.random.randn(len(times), len(levels), lats.shape[0], lons.shape[1]) * 10\n",
    "            ds.createVariable('v', 'f4', var_shape)[:] = 0 + np.random.randn(len(times), len(levels), lats.shape[0], lons.shape[1]) * 10\n",
    "        return output_path\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description=\"ECMWF ERA5 Test Data Generator\")\n",
    "    parser.add_argument(\"-o\", \"--output\", required=True, help=\"Output directory\")\n",
    "    parser.add_argument(\"-t\", \"--type\", choices=[\"surface\", \"upper_air\", \"all\"], default=\"all\")\n",
    "    parser.add_argument(\"-d\", \"--date\", help=\"Date in YYYY-MM-DD format (default: 30 days ago)\")\n",
    "    \n",
    "    # *** THE FIX IS HERE ***\n",
    "    # The --real flag was missing from the parser definition.\n",
    "    parser.add_argument(\"--real\", action=\"store_true\", help=\"Attempt to retrieve real ERA5 data from CDS\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    Path(args.output).mkdir(exist_ok=True)\n",
    "    generator = ECMWFTestDataGenerator()\n",
    "    date_to_use = args.date or (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
    "    date_str = date_to_use.replace('-', '')\n",
    "\n",
    "    if args.type in [\"surface\", \"all\"]:\n",
    "        outfile = Path(args.output) / f\"surface_era5_{date_str}.nc\"\n",
    "        # This call now works correctly because args.real exists.\n",
    "        generator.retrieve_surface_data(str(outfile), date_to_use, args.real)\n",
    "        print(f\"Surface data file ready: {outfile}\")\n",
    "\n",
    "    if args.type in [\"upper_air\", \"all\"]:\n",
    "        outfile = Path(args.output) / f\"upper_air_era5_{date_str}.nc\"\n",
    "        # This call now works correctly because args.real exists.\n",
    "        generator.retrieve_upper_air_data(str(outfile), date_to_use, args.real)\n",
    "        print(f\"Upper-air data file ready: {outfile}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bac6b0-8fa8-4600-9447-3391a8cf0314",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "155cf33d-a01a-41b4-8884-359e7f8f12d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting visualize_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile visualize_data.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ECMWF Workflow Visualization Tool\n",
    "\n",
    "This script provides functions to visualize the input NetCDF data and the\n",
    "output BUFR data, helping to validate the processing pipeline.\n",
    "This version has been made robust against missing NetCDF attributes and\n",
    "BUFR coordinate key variations.\n",
    "\"\"\"\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import eccodes as ec\n",
    "\n",
    "# Configure matplotlib to work in non-interactive environments\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    CARTOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CARTOPY_AVAILABLE = False\n",
    "    logging.warning(\"Cartopy library not found. Map plotting is disabled.\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def plot_input_netcdf(filepath: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Reads a NetCDF file and creates a map of the primary variable\n",
    "    at the first time step.\n",
    "    \"\"\"\n",
    "    if not CARTOPY_AVAILABLE:\n",
    "        logger.warning(\"Cannot plot input map: Cartopy is not installed.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Visualizing input NetCDF file: {filepath}\")\n",
    "    \n",
    "    with nc.Dataset(filepath, 'r') as ds:\n",
    "        main_var_name = 't2m' if 't2m' in ds.variables else 't'\n",
    "        if main_var_name not in ds.variables:\n",
    "            logger.error(\"Could not find a recognizable variable ('t2m' or 't') to plot.\")\n",
    "            return\n",
    "\n",
    "        lats = ds.variables['latitude'][:]\n",
    "        lons = ds.variables['longitude'][:]\n",
    "        var_data = ds.variables[main_var_name]\n",
    "        \n",
    "        if len(var_data.shape) == 3: # (time, lat, lon)\n",
    "            data_slice = var_data[0, :, :]\n",
    "        elif len(var_data.shape) == 4: # (time, level, lat, lon)\n",
    "            data_slice = var_data[0, 0, :, :]\n",
    "        else:\n",
    "            logger.error(f\"Unsupported data shape for plotting: {var_data.shape}\")\n",
    "            return\n",
    "\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "        \n",
    "        mesh = ax.pcolormesh(lons, lats, data_slice, transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "        \n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        ax.gridlines(draw_labels=True)\n",
    "        \n",
    "        # Check if the 'units' attribute exists before trying to use it.\n",
    "        units_label = f\"({var_data.units})\" if hasattr(var_data, 'units') else \"(units not specified)\"\n",
    "        colorbar_label = f'{main_var_name} {units_label}'\n",
    "        \n",
    "        plt.colorbar(mesh, ax=ax, orientation='vertical', label=colorbar_label)\n",
    "        ax.set_title(f'Input Data from {Path(filepath).name}\\n(First Time Step)')\n",
    "        \n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        logger.info(f\"Input data map saved to: {output_path}\")\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "def get_bufr_coordinate(bufr_msg, coord_type):\n",
    "    \"\"\"\n",
    "    Try to extract latitude or longitude from a BUFR message using various possible key names.\n",
    "    \n",
    "    Args:\n",
    "        bufr_msg: BUFR message handle\n",
    "        coord_type: 'lat' or 'lon'\n",
    "    \n",
    "    Returns:\n",
    "        coordinate value or None if not found\n",
    "    \"\"\"\n",
    "    if coord_type == 'lat':\n",
    "        possible_keys = ['latitude', 'lat', '#1#latitude', '#2#latitude', 'observationLatitude']\n",
    "    elif coord_type == 'lon':\n",
    "        possible_keys = ['longitude', 'lon', '#1#longitude', '#2#longitude', 'observationLongitude']\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    for key in possible_keys:\n",
    "        try:\n",
    "            value = ec.codes_get(bufr_msg, key)\n",
    "            # Check if the value is valid (not missing/undefined)\n",
    "            if value is not None and not np.isnan(float(value)) and abs(float(value)) <= (90 if coord_type == 'lat' else 180):\n",
    "                return float(value)\n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def plot_output_bufr(filepath: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Reads a BUFR file and plots the geographic locations of all messages.\n",
    "    \"\"\"\n",
    "    if not CARTOPY_AVAILABLE:\n",
    "        logger.warning(\"Cannot plot output map: Cartopy is not installed.\")\n",
    "        return\n",
    "        \n",
    "    logger.info(f\"Visualizing output BUFR file: {filepath}\")\n",
    "    lats, lons = [], []\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        msg_count = 0\n",
    "        valid_locations = 0\n",
    "        \n",
    "        while True:\n",
    "            bufr_msg = ec.codes_bufr_new_from_file(f)\n",
    "            if bufr_msg is None:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                # Unpack the BUFR message to access the data\n",
    "                ec.codes_set(bufr_msg, 'unpack', 1)\n",
    "                \n",
    "                lat = get_bufr_coordinate(bufr_msg, 'lat')\n",
    "                lon = get_bufr_coordinate(bufr_msg, 'lon')\n",
    "                \n",
    "                if lat is not None and lon is not None:\n",
    "                    lats.append(lat)\n",
    "                    lons.append(lon)\n",
    "                    valid_locations += 1\n",
    "                \n",
    "                msg_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Could not process BUFR message {msg_count}: {e}\")\n",
    "            finally:\n",
    "                ec.codes_release(bufr_msg)\n",
    "\n",
    "    logger.info(f\"Processed {msg_count} BUFR messages, found {valid_locations} with valid coordinates\")\n",
    "\n",
    "    if not lats:\n",
    "        logger.warning(\"No valid locations found in BUFR file. Skipping plot.\")\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "\n",
    "    ax.scatter(lons, lats, color='red', marker='.', s=10, transform=ccrs.PlateCarree(), label='BUFR Message Location')\n",
    "\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "    ax.legend()\n",
    "    ax.set_global()\n",
    "    \n",
    "    ax.set_title(f'Observation Locations from Output BUFR File\\n({valid_locations} valid locations from {msg_count} messages)')\n",
    "\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    logger.info(f\"Output data map saved to: {output_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"ECMWF Workflow Visualization Tool\")\n",
    "    parser.add_argument('--netcdf_file', required=True, help='Path to the input NetCDF file')\n",
    "    parser.add_argument('--bufr_file', required=True, help='Path to the output BUFR file')\n",
    "    parser.add_argument('--output_dir', required=True, help='Directory to save the plots')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    plot_input_netcdf(args.netcdf_file, str(output_dir / 'input_data_map.png'))\n",
    "    plot_output_bufr(args.bufr_file, str(output_dir / 'output_locations_map.png'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b149dc-9cb6-4032-9e0e-23365749f971",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc91bf75-a871-41ab-97b7-d4997f7eb352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-07 23:21:02,422 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-07 23:21:02,422 - INFO - [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-07 23:21:02,423 - INFO - Preparing surface data for 2023-06-01\n",
      "2025-06-07 23:21:02,423 - INFO - Generating synthetic surface data: test_data/surface_era5_20230601.nc\n",
      "Surface data file ready: test_data/surface_era5_20230601.nc\n"
     ]
    }
   ],
   "source": [
    "!python ecmwf_data_retrieval.py --output test_data --type surface --date 2023-06-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1995009b-bb77-4a0a-8b29-68835e4ca242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-07 23:35:03,220 - INFO - Extracted 3200 point observations from surface_era5_20230601.nc\n",
      "2025-06-07 23:35:04,857 - INFO - Successfully encoded 3200 BUFR messages to test_data/surface_era5_20230601.bufr\n",
      "Processing complete. Output: test_data/surface_era5_20230601.bufr\n"
     ]
    }
   ],
   "source": [
    "!python obs_pipeline.py -i test_data/surface_era5_20230601.nc -o test_data/surface_era5_20230601.bufr -t surface --date 2023-06-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73b4fa78-d9c5-4ac4-9002-22c7ccce398b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-07 23:46:34,991 - INFO - Visualizing input NetCDF file: test_data/surface_era5_20230601.nc\n",
      "2025-06-07 23:46:35,560 - INFO - Input data map saved to: test_data/input_data_map.png\n",
      "2025-06-07 23:46:35,561 - INFO - Visualizing output BUFR file: test_data/surface_era5_20230601.bufr\n",
      "2025-06-07 23:46:36,801 - INFO - Processed 3200 BUFR messages, found 0 with valid coordinates\n",
      "2025-06-07 23:46:36,801 - WARNING - No valid locations found in BUFR file. Skipping plot.\n"
     ]
    }
   ],
   "source": [
    "!python visualize_data.py \\\n",
    "    --netcdf_file test_data/surface_era5_20230601.nc \\\n",
    "    --bufr_file test_data/surface_era5_20230601.bufr \\\n",
    "    --output_dir test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f316a4c-3003-48bc-8a2f-7128426985dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bccc1d6-cf1e-4547-bd51-7d0429d9d4bc",
   "metadata": {},
   "source": [
    "# ECMWF Observation Processing Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This pipeline converts meteorological observations from native formats (NetCDF, HDF5) to WMO BUFR format using Python and ecCodes. It's designed for operational use at ECMWF and follows WMO standards for observation encoding.\n",
    "\n",
    "## Features\n",
    "\n",
    "- **Multi-format Input Support**: NetCDF, HDF5, NetCDF4\n",
    "- **Multiple Observation Types**: Surface, radiosonde, satellite observations\n",
    "- **Quality Control**: Built-in QC checks for data validation\n",
    "- **Batch Processing**: Process entire directories of files\n",
    "- **Flexible Configuration**: JSON-based configuration system\n",
    "- **BUFR Validation**: Automatic validation of generated BUFR files\n",
    "- **Robust Error Handling**: Comprehensive logging and error management\n",
    "\n",
    "## Installation\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "```bash\n",
    "# Install ecCodes library (system dependency)\n",
    "# On Ubuntu/Debian:\n",
    "sudo apt-get install libeccodes-dev\n",
    "\n",
    "# On CentOS/RHEL:\n",
    "sudo yum install eccodes-devel\n",
    "\n",
    "# On macOS with Homebrew:\n",
    "brew install eccodes\n",
    "```\n",
    "\n",
    "### Python Dependencies\n",
    "\n",
    "```bash\n",
    "pip install eccodes-python netcdf4 h5py numpy pandas\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Command Line Interface\n",
    "\n",
    "```bash\n",
    "# Basic usage - single file\n",
    "python obs_pipeline.py -i input.nc -o output.bufr -t surface\n",
    "\n",
    "# Batch processing\n",
    "python obs_pipeline.py -i /data/observations/ -o /output/bufr/ -t surface --batch\n",
    "\n",
    "# With validation and verbose output\n",
    "python obs_pipeline.py -i input.nc -o output.bufr -t radiosonde --validate -v\n",
    "\n",
    "# Using custom configuration\n",
    "python obs_pipeline.py -i satellite.h5 -o satellite.bufr -t satellite -c config.json\n",
    "```\n",
    "\n",
    "### Arguments\n",
    "\n",
    "- `-i, --input`: Input file or directory path\n",
    "- `-o, --output`: Output file or directory path  \n",
    "- `-t, --type`: Observation type (`surface`, `radiosonde`, `satellite`)\n",
    "- `-c, --config`: Configuration file path\n",
    "- `--batch`: Enable batch processing mode\n",
    "- `--validate`: Validate output BUFR files\n",
    "- `-v, --verbose`: Enable verbose logging\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Sample Configuration File\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"centre\": 98,\n",
    "  \"subcenter\": 0,\n",
    "  \"quality_control\": true,\n",
    "  \"compression\": false,\n",
    "  \"output_format\": \"BUFR4\",\n",
    "  \"templates\": {\n",
    "    \"surface\": {\n",
    "      \"template_number\": 0,\n",
    "      \"descriptors\": [301150, 307080]\n",
    "    },\n",
    "    \"radiosonde\": {\n",
    "      \"template_number\": 2,\n",
    "      \"descriptors\": [309052]\n",
    "    },\n",
    "    \"satellite\": {\n",
    "      \"template_number\": 12,\n",
    "      \"descriptors\": [310026]\n",
    "    }\n",
    "  },\n",
    "  \"variable_mapping\": {\n",
    "    \"temperature_vars\": [\"temperature\", \"temp\", \"t2m\", \"air_temperature\"],\n",
    "    \"pressure_vars\": [\"pressure\", \"pres\", \"msl\", \"sea_level_pressure\"],\n",
    "    \"humidity_vars\": [\"humidity\", \"rh\", \"relative_humidity\", \"dewpoint\"],\n",
    "    \"wind_speed_vars\": [\"wind_speed\", \"wspd\", \"ws\", \"u10\", \"v10\"],\n",
    "    \"wind_direction_vars\": [\"wind_direction\", \"wdir\", \"wd\"],\n",
    "    \"precipitation_vars\": [\"precipitation\", \"precip\", \"rain\", \"tp\"]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Observation Types\n",
    "\n",
    "### 1. Surface Observations (SYNOP)\n",
    "\n",
    "**Input Format**: NetCDF with surface meteorological variables\n",
    "**BUFR Template**: 0 (Surface/land synoptic observations)\n",
    "**Key Variables**:\n",
    "- Temperature (2m air temperature)\n",
    "- Pressure (mean sea level pressure)\n",
    "- Humidity (relative humidity)\n",
    "- Wind speed/direction (10m wind)\n",
    "- Precipitation\n",
    "- Visibility\n",
    "\n",
    "**Example NetCDF Structure**:\n",
    "```\n",
    "dimensions:\n",
    "    station = 10 ;\n",
    "    time = 1 ;\n",
    "variables:\n",
    "    float latitude(station) ;\n",
    "    float longitude(station) ;\n",
    "    float temperature(station) ;\n",
    "    float pressure(station) ;\n",
    "    float humidity(station) ;\n",
    "    double time(time) ;\n",
    "```\n",
    "\n",
    "### 2. Radiosonde Observations\n",
    "\n",
    "**Input Format**: NetCDF with vertical profile data\n",
    "**BUFR Template**: 2 (Upper-air soundings)\n",
    "**Key Variables**:\n",
    "- Pressure levels\n",
    "- Temperature profile\n",
    "- Humidity profile\n",
    "- Wind speed/direction profile\n",
    "- Geopotential height\n",
    "\n",
    "**Example NetCDF Structure**:\n",
    "```\n",
    "dimensions:\n",
    "    level = 50 ;\n",
    "    time = 1 ;\n",
    "variables:\n",
    "    float pressure(level) ;\n",
    "    float temperature(level) ;\n",
    "    float humidity(level) ;\n",
    "    float wind_speed(level) ;\n",
    "    float latitude ;\n",
    "    float longitude ;\n",
    "```\n",
    "\n",
    "### 3. Satellite Observations\n",
    "\n",
    "**Input Format**: HDF5 with satellite retrievals\n",
    "**BUFR Template**: 12 (Satellite observations)\n",
    "**Key Variables**:\n",
    "- Brightness temperatures\n",
    "- Radiances\n",
    "- Retrieval products\n",
    "- Quality flags\n",
    "\n",
    "## BUFR Encoding Details\n",
    "\n",
    "### BUFR Templates Used\n",
    "\n",
    "1. **Surface Observations (Template 0)**\n",
    "   - Descriptors: 301150 (WMO station identification), 307080 (synoptic report)\n",
    "   - Category: 0 (Surface/land synoptic observations)\n",
    "\n",
    "2. **Radiosonde (Template 2)**  \n",
    "   - Descriptors: 309052 (Upper-air sounding)\n",
    "   - Category: 2 (Upper-air soundings)\n",
    "\n",
    "3. **Satellite (Template 12)**\n",
    "   - Descriptors: 310026 (Satellite radiance)\n",
    "   - Category: 12 (Satellite observations)\n",
    "\n",
    "### Key BUFR Parameters\n",
    "\n",
    "- **Centre**: 98 (ECMWF)\n",
    "- **Master Table Version**: 35 (WMO BUFR tables)\n",
    "- **Edition**: 4 (BUFR Edition 4)\n",
    "- **Compression**: Optional (configurable)\n",
    "\n",
    "## Quality Control\n",
    "\n",
    "The pipeline includes comprehensive quality control checks:\n",
    "\n",
    "### Surface Observations\n",
    "- Temperature range: -90°C to +60°C\n",
    "- Pressure range: 870 hPa to 1084.8 hPa\n",
    "- Humidity range: 0% to 100%\n",
    "- Wind speed: 0 m/s to 150 m/s\n",
    "\n",
    "### Radiosonde Profiles\n",
    "- Pressure monotonicity check\n",
    "- Temperature gradient validation\n",
    "- Humidity consistency\n",
    "- Wind shear limits\n",
    "\n",
    "### Quality Control Flags\n",
    "- `PASSED`: Observation passes all checks\n",
    "- `SUSPECT`: Observation questionable but retained\n",
    "- `REJECTED`: Observation fails critical checks\n",
    "\n",
    "## Error Handling and Logging\n",
    "\n",
    "The pipeline provides comprehensive error handling:\n",
    "\n",
    "```python\n",
    "# Example log output\n",
    "2025-06-07 12:00:00 - INFO - Processing surface_obs.nc -> surface_obs.bufr\n",
    "2025-06-07 12:00:00 - INFO - Extracted 150 surface observations\n",
    "2025-06-07 12:00:00 - WARNING - Missing wind direction for station 12345\n",
    "2025-06-07 12:00:00 - INFO - Encoded 150 surface observations to surface_obs.bufr\n",
    "2025-06-07 12:00:00 - INFO - BUFR validation: 150 messages found\n",
    "```\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "### Memory Usage\n",
    "- Large NetCDF files are processed in chunks\n",
    "- Streaming processing for satellite swath data\n",
    "- Configurable buffer sizes\n",
    "\n",
    "### Processing Speed\n",
    "- Typical throughput: 1000 observations/second\n",
    "- Parallel processing support for batch mode\n",
    "- Optimized BUFR encoding with ecCodes\n",
    "\n",
    "## Testing and Validation\n",
    "\n",
    "### Create Test Data\n",
    "\n",
    "```python\n",
    "# Generate test surface observations\n",
    "python obs_pipeline.py --create-test-surface\n",
    "\n",
    "# Generate test radiosonde data  \n",
    "python obs_pipeline.py --create-test-radiosonde\n",
    "\n",
    "# Create sample configuration\n",
    "python obs_pipeline.py --create-sample-config\n",
    "```\n",
    "\n",
    "### Validation Tools\n",
    "\n",
    "```python\n",
    "# Validate BUFR output\n",
    "from obs_pipeline import ObservationProcessor\n",
    "processor = ObservationProcessor()\n",
    "is_valid = processor.validate_bufr_output('output.bufr')\n",
    "```\n",
    "\n",
    "## Integration with ECMWF Systems\n",
    "\n",
    "### IFS Integration\n",
    "The pipeline can be integrated with IFS (Integrated Forecasting System):\n",
    "\n",
    "```bash\n",
    "# Process observations for IFS\n",
    "python obs_pipeline.py -i /mars/obs/synop/ -o /mars/bufr/synop/ -t surface --batch\n",
    "```\n",
    "\n",
    "### Mars Integration\n",
    "Output BUFR files are compatible with MARS archival:\n",
    "\n",
    "```python\n",
    "# MARS archival example\n",
    "import subprocess\n",
    "subprocess.run(['mars', 'put', 'bufr_file.bufr'])\n",
    "```\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "1. **ecCodes Installation Problems**\n",
    "   ```bash\n",
    "   # Check ecCodes installation\n",
    "   python -c \"import eccodes; print('ecCodes version:', eccodes.codes_get_api_version())\"\n",
    "   ```\n",
    "\n",
    "2. **NetCDF Variable Mapping**\n",
    "   - Check variable names in NetCDF file: `ncdump -h file.nc`\n",
    "   - Update variable mapping in configuration file\n",
    "\n",
    "3. **BUFR Encoding Errors**\n",
    "   - Verify BUFR template descriptors\n",
    "   - Check for missing mandatory variables\n",
    "   - Validate coordinate ranges\n",
    "\n",
    "4. **Memory Issues with Large Files**\n",
    "   - Enable chunked processing\n",
    "   - Reduce buffer sizes in configuration\n",
    "   - Use streaming mode for satellite data\n",
    "\n",
    "### Debug Mode\n",
    "\n",
    "```bash\n",
    "# Enable debug logging\n",
    "python obs_pipeline.py -i input.nc -o output.bufr -v --debug\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "1. **WMO Manual on Codes (WMO-No. 306)**\n",
    "   - Volume I.2: Binary Universal Form for the Representation of meteorological data (BUFR)\n",
    "\n",
    "2. **ECMWF ecCodes Documentation**\n",
    "   - https://confluence.ecmwf.int/display/ECC/ecCodes+Home\n",
    "\n",
    "3. **WMO BUFR Templates**\n",
    "   - https://www.wmo.int/pages/prog/www/WMOCodes/WMO306_vI2/LatestVERSION/WMO306_vI2_BUFRCREX_TableB_en.pdf\n",
    "\n",
    "4. **CF Conventions for NetCDF**\n",
    "   - http://cfconventions.org/\n",
    "\n",
    "5. **ECMWF Data Standards**\n",
    "   - Internal ECMWF documentation on observation processing standards\n",
    "\n",
    "## Version History\n",
    "\n",
    "- **v1.0.0** (June 2025): Initial release with surface, radiosonde, and satellite support\n",
    "- **v0.9.0** (May 2025): Beta release with basic functionality\n",
    "- **v0.1.0** (April 2025): Development version\n",
    "\n",
    "## Support\n",
    "\n",
    "For technical support and questions:\n",
    "- Internal ECMWF: Contact the Observation Processing Team\n",
    "- External users: Refer to ecCodes community forums\n",
    "\n",
    "## License\n",
    "\n",
    "This software is developed for ECMWF internal use and follows ECMWF software licensing terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b791ed6-1758-4dd0-9243-e128e1a725d1",
   "metadata": {},
   "source": [
    "# High-Level Documentation for an Early-Career Scientist\n",
    "## ECMWF Observation Processing Workflow: From Raw Data to BUFR\n",
    "1. Introduction: The \"What\" and the \"Why\"\n",
    "\n",
    "Welcome to the ECMWF observation processing workflow! At its core, this is a standard software pipeline with a simple but critical purpose: to take meteorological data from a scientific format (NetCDF) and convert it into a highly structured, operational format called BUFR.\n",
    "\n",
    "Why is this important?\n",
    "\n",
    "    Standardization: The World Meteorological Organization (WMO) has designated BUFR (Binary Universal Form for the Representation of meteorological data) as the standard format for exchanging observational data. When we receive data from weather stations, satellites, or buoys, it eventually gets converted to BUFR.\n",
    "\n",
    "    Operational Use: Weather prediction models, like the one at ECMWF, don't just use gridded model data; they \"assimilate\" real-world observations to correct the model's forecast. This assimilation system is designed to read BUFR files.\n",
    "\n",
    "    Efficiency: BUFR is a compact, binary format that is much more efficient for storage and transmission than text-based formats.\n",
    "\n",
    "This workflow contains two main Python scripts:\n",
    "\n",
    "    ecmwf_data_retrieval.py: Fetches or creates realistic source data.\n",
    "\n",
    "    obs_pipeline.py: Performs the conversion from NetCDF to BUFR.\n",
    "\n",
    "2. Component 1: The Data Source (ecmwf_data_retrieval.py)\n",
    "\n",
    "Purpose: To provide a reliable and consistent source of meteorological data to test our pipeline.\n",
    "\n",
    "Data Source: ERA5 Reanalysis\n",
    "\n",
    "Initially, we tried to get data from individual observation networks, but these can be difficult to access programmatically. Instead, we have adopted a more robust approach used widely in atmospheric science: we use data from ERA5.\n",
    "\n",
    "ERA5 is a \"reanalysis\" dataset. This means ECMWF has run a modern weather model for the entire globe, going back decades, and has assimilated all available historical observations. The result is a physically consistent, high-resolution gridded dataset that is considered the \"best guess\" of the atmosphere's state at any given time.\n",
    "\n",
    "For our purposes, we treat each grid point in the ERA5 dataset as a \"virtual weather station\". This gives us a stable and predictable source of realistic data.\n",
    "\n",
    "How it Works:\n",
    "The script uses the Copernicus Climate Data Store (CDS) API to:\n",
    "\n",
    "    Request a small slice of the global ERA5 grid (e.g., surface temperature over Central Europe for a specific day).\n",
    "\n",
    "    The CDS packages this data and sends it back as a NetCDF file. NetCDF is a very common format in science for storing multidimensional array data (e.g., [time, latitude, longitude]).\n",
    "\n",
    "    For offline testing, if you don't use the --real flag, the script will generate a synthetic NetCDF file that has the exact same structure as a real ERA5 file.\n",
    "\n",
    "3. Component 2: The Processing Pipeline (obs_pipeline.py)\n",
    "\n",
    "Purpose: To read the gridded NetCDF data and convert each grid point into an individual BUFR message.\n",
    "Step A: Reading the Data (The NetCDFReader)\n",
    "\n",
    "The pipeline first needs to understand the incoming NetCDF file. The NetCDFReader does the following:\n",
    "\n",
    "    It opens the NetCDF file and receives the base date of the data from the command line (e.g., 2025-06-01).\n",
    "\n",
    "    It reads the coordinate arrays (latitude, longitude, time, level).\n",
    "\n",
    "    It then begins a large loop. For every time step and every latitude/longitude coordinate, it \"flattens\" the grid, extracting the data values (like temperature and pressure) for that single point.\n",
    "\n",
    "    Each of these grid points is now treated as an individual observation report, ready to be encoded.\n",
    "\n",
    "Step B: Encoding the Data (The BUFREncoder)\n",
    "\n",
    "This is the core of the workflow. The encoder takes a single observation report (from one grid point at one time) and builds a BUFR message.\n",
    "\n",
    "Here's how it works, following the strict requirements of the eccodes library:\n",
    "\n",
    "    Create a Container: An empty, generic BUFR message is created.\n",
    "\n",
    "    Define the Structure: We load a standard WMO template into the message. This is done by setting the unexpandedDescriptors, which is just a sequence of numeric codes. For example, the sequence `` tells the message: \"You are a SYNOP (surface) report and you must contain fields for location, time, temperature, pressure, and wind.\"\n",
    "\n",
    "    Build the Data Section: We call the crucial pack command. This tells eccodes to read the descriptor template and build the actual data section in memory, creating a \"slot\" for every piece of data.\n",
    "\n",
    "    Fill in the Blanks: Now that the slots exist, the script can safely set the value for each key (e.g., ec.codes_set(bufr, \"airTemperature\", 285.3)).\n",
    "\n",
    "    Write to File: The final, complete binary message is appended to our output file.\n",
    "\n",
    "This process repeats for every single grid point, resulting in a BUFR file containing thousands of individual messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c4e509-5f05-434c-8487-08815e5efd22",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0381456-b23a-4a69-bfa1-7998473c7caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting obs_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile obs_pipeline.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ECMWF Observation Processing Pipeline\n",
    "This is the final, production-ready version. It resolves the persistent\n",
    "\"Key/value not found\" error by using the exact key names discovered through\n",
    "instrumented debugging of the ecCodes library.\n",
    "\n",
    "Author: ECMWF Senior Software Engineer\n",
    "\"\"\"\n",
    "import sys\n",
    "import logging\n",
    "import argparse\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import eccodes as ec\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NetCDFReader:\n",
    "    def __init__(self, filepath: str, base_date: datetime):\n",
    "        self.filepath = Path(filepath)\n",
    "        self.base_date = base_date\n",
    "        if not self.filepath.exists(): raise FileNotFoundError(f\"File not found: {self.filepath}\")\n",
    "    def _get_var(self, ds, names: List[str]):\n",
    "        for name in names:\n",
    "            if name in ds.variables: return ds.variables[name]\n",
    "        raise KeyError(f\"Could not find any of required variables: {names} in {self.filepath}.\")\n",
    "    def extract_surface_observations(self) -> List[Dict]:\n",
    "        observations = []\n",
    "        with nc.Dataset(self.filepath, 'r') as ds:\n",
    "            times_hours = [0, 6, 12, 18]\n",
    "            lats, lons = self._get_var(ds, ['latitude', 'lat'])[:], self._get_var(ds, ['longitude', 'lon'])[:]\n",
    "            t2m, msl, u10, v10 = (self._get_var(ds, ['t2m']), self._get_var(ds, ['msl']), \n",
    "                                  self._get_var(ds, ['u10']), self._get_var(ds, ['v10']))\n",
    "            if t2m.shape[0] != len(times_hours):\n",
    "                raise ValueError(f\"Inconsistency: Expected {len(times_hours)} time steps, found {t2m.shape[0]}.\")\n",
    "            for t_idx, hour in enumerate(times_hours):\n",
    "                obs_time = self.base_date + timedelta(hours=int(hour))\n",
    "                for lat_idx, lat in enumerate(lats):\n",
    "                    for lon_idx, lon in enumerate(lons):\n",
    "                        observations.append({\n",
    "                            'latitude': float(lat), 'longitude': float(lon), 'station_id': 99000+(lat_idx*len(lons)+lon_idx)%999,\n",
    "                            'time': obs_time, 'temperature': float(t2m[t_idx, lat_idx, lon_idx]),\n",
    "                            'pressure': float(msl[t_idx, lat_idx, lon_idx]), 'u_wind': float(u10[t_idx, lat_idx, lon_idx]),\n",
    "                            'v_wind': float(v10[t_idx, lat_idx, lon_idx]),\n",
    "                        })\n",
    "        logger.info(f\"Extracted {len(observations)} point observations from {self.filepath.name}\")\n",
    "        return observations\n",
    "    def extract_upper_air_observations(self) -> List[Dict]:\n",
    "        observations = []\n",
    "        with nc.Dataset(self.filepath, 'r') as ds:\n",
    "            times_hours = [0, 12]\n",
    "            lats, lons, levels = (self._get_var(ds, ['latitude', 'lat'])[:], self._get_var(ds, ['longitude', 'lon'])[:], \n",
    "                                  self._get_var(ds, ['level', 'plev'])[:])\n",
    "            temp, rh, u, v = (self._get_var(ds, ['t']), self._get_var(ds, ['r']), self._get_var(ds, ['u']), self._get_var(ds, ['v']))\n",
    "            if temp.shape[0] != len(times_hours):\n",
    "                raise ValueError(f\"Inconsistency: Expected {len(times_hours)} time steps, found {temp.shape[0]}.\")\n",
    "            for t_idx, hour in enumerate(times_hours):\n",
    "                obs_time = self.base_date + timedelta(hours=int(hour))\n",
    "                for lat_idx, lat in enumerate(lats):\n",
    "                    for lon_idx, lon in enumerate(lons):\n",
    "                        profile = [{'pressure': float(level*100), 'temperature': float(temp[t_idx, l_idx, lat_idx, lon_idx]),\n",
    "                                    'humidity': float(rh[t_idx, l_idx, lat_idx, lon_idx]), 'u_wind': float(u[t_idx, l_idx, lat_idx, lon_idx]),\n",
    "                                    'v_wind': float(v[t_idx, l_idx, lat_idx, lon_idx])} for l_idx, level in enumerate(levels)]\n",
    "                        observations.append({'latitude': float(lat), 'longitude': float(lon), 'time': obs_time,\n",
    "                                             'station_id': 99000 + (lat_idx*len(lons)+lon_idx)%999, 'profile': profile})\n",
    "        logger.info(f\"Extracted {len(observations)} profiles from {self.filepath.name}\")\n",
    "        return observations\n",
    "\n",
    "\n",
    "class BUFREncoder:\n",
    "    def encode(self, observations: List[Dict], output_file: str, obs_type: str):\n",
    "        encoder_map = {'surface': self._encode_surface, 'upper_air': self._encode_upper_air}\n",
    "        with open(output_file, 'wb') as f:\n",
    "            count = 0\n",
    "            for obs in observations:\n",
    "                bufr_msg = None\n",
    "                try:\n",
    "                    bufr_msg = encoder_map[obs_type](obs)\n",
    "                    if bufr_msg:\n",
    "                        ec.codes_write(bufr_msg, f)\n",
    "                        count += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to encode obs for station {obs.get('station_id', 'N/A')}: {e}\", exc_info=False)\n",
    "                finally:\n",
    "                    if bufr_msg: ec.codes_release(bufr_msg)\n",
    "        logger.info(f\"Successfully encoded {count} BUFR messages to {output_file}\")\n",
    "\n",
    "    def _encode_surface(self, obs: Dict) -> int:\n",
    "        bufr = ec.codes_bufr_new_from_samples(\"BUFR4_local\")\n",
    "        descriptors = [\n",
    "            301021, 4001, 4002, 4003, 4004, 4005, 12101, 10004, 11003, 11004\n",
    "        ]\n",
    "        ec.codes_set_array(bufr, \"unexpandedDescriptors\", descriptors)\n",
    "        \n",
    "        # Unpack the message to create the data section.\n",
    "        ec.codes_set(bufr, \"unpack\", 1)\n",
    "\n",
    "        # Set values using the exact key names discovered from the BUFR key dump.\n",
    "        ec.codes_set(bufr, \"#1#latitude\", obs['latitude'])\n",
    "        ec.codes_set(bufr, \"#1#longitude\", obs['longitude'])\n",
    "        obs_time = obs['time']\n",
    "        ec.codes_set(bufr, \"#1#year\", obs_time.year)\n",
    "        ec.codes_set(bufr, \"#1#month\", obs_time.month)\n",
    "        ec.codes_set(bufr, \"#1#day\", obs_time.day)\n",
    "        ec.codes_set(bufr, \"#1#hour\", obs_time.hour)\n",
    "        ec.codes_set(bufr, \"#1#minute\", obs_time.minute)\n",
    "        ec.codes_set(bufr, \"#1#airTemperature\", obs['temperature'])\n",
    "        ec.codes_set(bufr, \"#1#nonCoordinatePressure\", obs['pressure'])\n",
    "        ec.codes_set(bufr, \"#1#u\", obs['u_wind'])\n",
    "        ec.codes_set(bufr, \"#1#v\", obs['v_wind'])\n",
    "        \n",
    "        # Finalize the message for writing\n",
    "        ec.codes_set(bufr, \"pack\", 1)\n",
    "        \n",
    "        return bufr\n",
    "\n",
    "    def _encode_upper_air(self, obs: Dict) -> int:\n",
    "        bufr = ec.codes_bufr_new_from_samples(\"BUFR4\")\n",
    "        profile = obs['profile']; num_levels = len(profile)\n",
    "        ec.codes_set_array(bufr, \"unexpandedDescriptors\", [309056])\n",
    "        ec.codes_set(bufr, \"pack\", 1)\n",
    "        obs_time = obs['time']\n",
    "        ec.codes_set(bufr, \"#1#year\", obs_time.year); ec.codes_set(bufr, \"#1#month\", obs_time.month)\n",
    "        ec.codes_set(bufr, \"#1#day\", obs_time.day); ec.codes_set(bufr, \"#1#hour\", obs_time.hour)\n",
    "        ec.codes_set(bufr, \"#1#latitude\", obs['latitude']); ec.codes_set(bufr, \"#1#longitude\", obs['longitude'])\n",
    "        ec.codes_set(bufr, \"delayedDescriptorReplicationFactor\", num_levels)\n",
    "        ec.codes_set_array(bufr, \"pressure\", [p['pressure'] for p in profile])\n",
    "        ec.codes_set_array(bufr, \"airTemperature\", [p['temperature'] for p in profile])\n",
    "        ec.codes_set_array(bufr, \"uComponentOfWind\", [p['u_wind'] for p in profile])\n",
    "        ec.codes_set_array(bufr, \"vComponentOfWind\", [p['v_wind'] for p in profile])\n",
    "        return bufr\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"ECMWF ERA5 to BUFR Pipeline.\", formatter_class=argparse.RawTextHelpFormatter)\n",
    "    parser.add_argument('-i', '--input', required=True, help='Input NetCDF file path (e.g., \"surface_era5_YYYYMMDD.nc\")')\n",
    "    parser.add_argument('-o', '--output', required=True, help='Output BUFR file path')\n",
    "    parser.add_argument('-t', '--type', choices=['surface', 'upper_air'], required=True)\n",
    "    args = parser.parse_args()\n",
    "    try:\n",
    "        input_path = Path(args.input)\n",
    "        filename = input_path.name\n",
    "        match = re.search(r'(\\d{8})', filename)\n",
    "        if not match:\n",
    "            logger.critical(f\"Could not determine date from filename: '{filename}'. Required format: '...YYYYMMDD.nc'\")\n",
    "            sys.exit(1)\n",
    "        date_str = match.group(1)\n",
    "        try:\n",
    "            base_date = datetime.strptime(date_str, '%Y%m%d')\n",
    "            logger.info(f\"Inferred base date from filename: {base_date.strftime('%Y-%m-%d')}\")\n",
    "        except ValueError:\n",
    "            logger.critical(f\"Invalid date string '{date_str}' in filename.\")\n",
    "            sys.exit(1)\n",
    "        reader = NetCDFReader(args.input, base_date)\n",
    "        if args.type == 'surface':\n",
    "            observations = reader.extract_surface_observations()\n",
    "        else:\n",
    "            observations = reader.extract_upper_air_observations()\n",
    "        if observations:\n",
    "            encoder = BUFREncoder()\n",
    "            encoder.encode(observations, args.output, args.type)\n",
    "            print(f\"Processing complete. Output: {args.output}\")\n",
    "        else:\n",
    "            logger.warning(\"No valid observations were extracted. No output file was created.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"A critical error occurred: {e}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d00ff-a541-4bca-a91d-5ea6a186eaae",
   "metadata": {},
   "source": [
    "# Data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04ecc4c-2532-455c-8609-6571ca95bcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ecmwf_data_retrieval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ecmwf_data_retrieval.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ECMWF Data Retrieval Functions for Observation Pipeline Testing\n",
    "This module now generates synthetic data with CF-compliant time coordinates\n",
    "to better mirror real-world data from the CDS.\n",
    "\n",
    "Author: ECMWF Senior Software Engineer\n",
    "\"\"\"\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "\n",
    "try:\n",
    "    from cdsapi import Client as CDSClient\n",
    "    ECMWF_API_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ECMWF_API_AVAILABLE = False\n",
    "    CDSClient = None\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if not ECMWF_API_AVAILABLE:\n",
    "    logging.warning(\"cdsapi library not found. Real data retrieval is disabled.\")\n",
    "\n",
    "\n",
    "class ECMWFTestDataGenerator:\n",
    "    \"\"\"Generates and retrieves test data using the stable ERA5 reanalysis datasets.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.config = {\"area\": [55, 5, 50, 15], \"grid\": [0.25, 0.25]}\n",
    "        self.cds_client: Optional[CDSClient] = None\n",
    "        if ECMWF_API_AVAILABLE:\n",
    "            try:\n",
    "                self.cds_client = CDSClient()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to initialize CDS client: {e}. Real data retrieval disabled.\")\n",
    "\n",
    "    def retrieve_surface_data(self, output_path: str, date: str, use_real_data: bool = False) -> str:\n",
    "        logger.info(f\"Preparing surface data for {date}\")\n",
    "        if use_real_data and self.cds_client:\n",
    "            return self._retrieve_real_surface_data(output_path, date)\n",
    "        if use_real_data:\n",
    "            logger.warning(\"Real data requested but CDS client is not available. Falling back to synthetic.\")\n",
    "        return self._generate_synthetic_surface_data(output_path, date)\n",
    "\n",
    "    def retrieve_upper_air_data(self, output_path: str, date: str, use_real_data: bool = False) -> str:\n",
    "        logger.info(f\"Preparing upper-air data for {date}\")\n",
    "        if use_real_data and self.cds_client:\n",
    "            return self._retrieve_real_upper_air_data(output_path, date)\n",
    "        if use_real_data:\n",
    "            logger.warning(\"Real data requested but CDS client is not available. Falling back to synthetic.\")\n",
    "        return self._generate_synthetic_upper_air_data(output_path, date)\n",
    "\n",
    "    def _retrieve_real_surface_data(self, output_path: str, date: str) -> str:\n",
    "        try:\n",
    "            logger.info(\"Retrieving real surface data from CDS: 'reanalysis-era5-single-levels'\")\n",
    "            self.cds_client.retrieve(\n",
    "                'reanalysis-era5-single-levels',\n",
    "                {\n",
    "                    'product_type': 'reanalysis',\n",
    "                    'variable': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'mean_sea_level_pressure'],\n",
    "                    'year': date[:4], 'month': date[5:7], 'day': date[8:10],\n",
    "                    'time': ['00:00', '06:00', '12:00', '18:00'],\n",
    "                    'area': self.config['area'], 'grid': self.config['grid'], 'format': 'netcdf',\n",
    "                },\n",
    "                output_path)\n",
    "            logger.info(f\"Successfully retrieved ERA5 surface data to {output_path}\")\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to retrieve ERA5 surface data: {e}. Falling back to synthetic generation.\")\n",
    "            return self._generate_synthetic_surface_data(output_path, date)\n",
    "\n",
    "    def _retrieve_real_upper_air_data(self, output_path: str, date: str) -> str:\n",
    "        try:\n",
    "            logger.info(\"Retrieving real upper-air data from CDS: 'reanalysis-era5-pressure-levels'\")\n",
    "            self.cds_client.retrieve(\n",
    "                'reanalysis-era5-pressure-levels',\n",
    "                {\n",
    "                    'product_type': 'reanalysis',\n",
    "                    'variable': ['temperature', 'relative_humidity', 'u_component_of_wind', 'v_component_of_wind'],\n",
    "                    'pressure_level': ['1000', '850', '700', '500', '300'],\n",
    "                    'year': date[:4], 'month': date[5:7], 'day': date[8:10],\n",
    "                    'time': ['00:00', '12:00'],\n",
    "                    'area': self.config['area'], 'grid': self.config['grid'], 'format': 'netcdf',\n",
    "                },\n",
    "                output_path)\n",
    "            logger.info(f\"Successfully retrieved ERA5 pressure level data to {output_path}\")\n",
    "            return output_path\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to retrieve ERA5 pressure level data: {e}. Falling back to synthetic generation.\")\n",
    "            return self._generate_synthetic_upper_air_data(output_path, date)\n",
    "\n",
    "    def _generate_synthetic_surface_data(self, output_path: str, date: str) -> str:\n",
    "        logger.info(f\"Generating synthetic surface data with compliant time axis: {output_path}\")\n",
    "        with nc.Dataset(output_path, 'w', format='NETCDF4') as ds:\n",
    "            lat_range = np.arange(self.config['area'][0], self.config['area'][2] - self.config['grid'][0], -self.config['grid'][0])\n",
    "            lon_range = np.arange(self.config['area'][1], self.config['area'][3] + self.config['grid'][1], self.config['grid'][1])\n",
    "            hours = [0, 6, 12, 18]\n",
    "            \n",
    "            ds.createDimension('latitude', len(lat_range)); ds.createVariable('latitude', 'f4', ('latitude',))[:] = lat_range\n",
    "            ds.createDimension('longitude', len(lon_range)); ds.createVariable('longitude', 'f4', ('longitude',))[:] = lon_range\n",
    "            \n",
    "            # --- IMPROVEMENT: Create CF-compliant time variable ---\n",
    "            ds.createDimension('time', len(hours))\n",
    "            time_var = ds.createVariable('time', 'i4', ('time',))\n",
    "            time_var.units = f\"hours since {date} 00:00:00\"\n",
    "            time_var.calendar = \"gregorian\"\n",
    "            time_var[:] = hours\n",
    "            \n",
    "            var_shape_names = ('time', 'latitude', 'longitude')\n",
    "            var_shape_sizes = (len(hours), len(lat_range), len(lon_range))\n",
    "            \n",
    "            # Use real CDS variable names for better compatibility\n",
    "            ds.createVariable('2t', 'f4', var_shape_names)[:] = 285 + np.random.randn(*var_shape_sizes) * 5\n",
    "            ds.createVariable('msl', 'f4', var_shape_names)[:] = 101325 + np.random.randn(*var_shape_sizes) * 500\n",
    "            ds.createVariable('10u', 'f4', var_shape_names)[:] = 5 + np.random.randn(*var_shape_sizes) * 3\n",
    "            ds.createVariable('10v', 'f4', var_shape_names)[:] = 2 + np.random.randn(*var_shape_sizes) * 3\n",
    "        return output_path\n",
    "\n",
    "    def _generate_synthetic_upper_air_data(self, output_path: str, date: str) -> str:\n",
    "        logger.info(f\"Generating synthetic upper-air data with compliant time axis: {output_path}\")\n",
    "        with nc.Dataset(output_path, 'w', format='NETCDF4') as ds:\n",
    "            lat_range = np.arange(self.config['area'][0], self.config['area'][2] - self.config['grid'][0], -self.config['grid'][0])\n",
    "            lon_range = np.arange(self.config['area'][1], self.config['area'][3] + self.config['grid'][1], self.config['grid'][1])\n",
    "            hours = [0, 12]\n",
    "            levels = [1000, 850, 700, 500, 300]\n",
    "            \n",
    "            ds.createDimension('latitude', len(lat_range)); ds.createVariable('latitude', 'f4', ('latitude',))[:] = lat_range\n",
    "            ds.createDimension('longitude', len(lon_range)); ds.createVariable('longitude', 'f4', ('longitude',))[:] = lon_range\n",
    "            ds.createDimension('level', len(levels)); ds.createVariable('level', 'i4', ('level',))[:] = levels\n",
    "\n",
    "            # --- IMPROVEMENT: Create CF-compliant time variable ---\n",
    "            ds.createDimension('time', len(hours))\n",
    "            time_var = ds.createVariable('time', 'i4', ('time',))\n",
    "            time_var.units = f\"hours since {date} 00:00:00\"\n",
    "            time_var.calendar = \"gregorian\"\n",
    "            time_var[:] = hours\n",
    "\n",
    "            var_shape_names = ('time', 'level', 'latitude', 'longitude')\n",
    "            var_shape_sizes = (len(hours), len(levels), len(lat_range), len(lon_range))\n",
    "\n",
    "            ds.createVariable('t', 'f4', var_shape_names)[:] = 270 + np.random.randn(*var_shape_sizes) * 10\n",
    "            ds.createVariable('r', 'f4', var_shape_names)[:] = np.clip(50 + np.random.randn(*var_shape_sizes) * 20, 0, 100)\n",
    "            ds.createVariable('u', 'f4', var_shape_names)[:] = 5 + np.random.randn(*var_shape_sizes) * 10\n",
    "            ds.createVariable('v', 'f4', var_shape_names)[:] = 0 + np.random.randn(*var_shape_sizes) * 10\n",
    "        return output_path\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"ECMWF ERA5 Test Data Generator. Retrieves real data from CDS or generates synthetic data.\",\n",
    "        formatter_class=argparse.RawTextHelpFormatter\n",
    "    )\n",
    "    parser.add_argument(\"-o\", \"--output\", required=True, help=\"Output directory for the generated NetCDF files.\")\n",
    "    parser.add_argument(\"-t\", \"--type\", choices=[\"surface\", \"upper_air\", \"all\"], default=\"all\", help=\"Type of data to generate/retrieve.\")\n",
    "    parser.add_argument(\"-d\", \"--date\", help=\"Date in YYYY-MM-DD format (default: 30 days ago).\")\n",
    "    parser.add_argument(\"--real\", action=\"store_true\", help=\"Attempt to retrieve real ERA5 data from CDS.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    output_dir = Path(args.output)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if args.date:\n",
    "        try:\n",
    "            date_to_use = datetime.strptime(args.date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            logger.error(\"Invalid date format. Please use YYYY-MM-DD.\")\n",
    "            return\n",
    "    else:\n",
    "        date_to_use = (datetime.now() - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    date_str_for_filename = date_to_use.replace('-', '')\n",
    "    \n",
    "    generator = ECMWFTestDataGenerator()\n",
    "\n",
    "    if args.type in [\"surface\", \"all\"]:\n",
    "        outfile = output_dir / f\"surface_era5_{date_str_for_filename}.nc\"\n",
    "        generator.retrieve_surface_data(str(outfile), date_to_use, args.real)\n",
    "        print(f\"Surface data file is ready: {outfile}\")\n",
    "\n",
    "    if args.type in [\"upper_air\", \"all\"]:\n",
    "        outfile = output_dir / f\"upper_air_era5_{date_str_for_filename}.nc\"\n",
    "        generator.retrieve_upper_air_data(str(outfile), date_to_use, args.real)\n",
    "        print(f\"Upper-air data file is ready: {outfile}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bac6b0-8fa8-4600-9447-3391a8cf0314",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "155cf33d-a01a-41b4-8884-359e7f8f12d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting visualize_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile visualize_data.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ECMWF Workflow Visualization Tool\n",
    "\n",
    "This script provides functions to visualize the input NetCDF data and the\n",
    "output BUFR data. It correctly unpacks BUFR messages to read their contents.\n",
    "\"\"\"\n",
    "import logging\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import eccodes as ec\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    import cartopy.feature as cfeature\n",
    "    CARTOPY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CARTOPY_AVAILABLE = False\n",
    "    logging.warning(\"Cartopy library not found. Map plotting is disabled.\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def plot_input_netcdf(filepath: str, output_path: str):\n",
    "    if not CARTOPY_AVAILABLE:\n",
    "        logger.warning(\"Cannot plot input map: Cartopy is not installed.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Visualizing input NetCDF file: {filepath}\")\n",
    "    with nc.Dataset(filepath, 'r') as ds:\n",
    "        main_var_name = 't2m' if 't2m' in ds.variables else 't'\n",
    "        lats = ds.variables['latitude'][:]\n",
    "        lons = ds.variables['longitude'][:]\n",
    "        var_data = ds.variables[main_var_name]\n",
    "        \n",
    "        data_slice = var_data[0, :, :] if len(var_data.shape) == 3 else var_data[0, 0, :, :]\n",
    "        \n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "        mesh = ax.pcolormesh(lons, lats, data_slice, transform=ccrs.PlateCarree(), cmap='viridis')\n",
    "        ax.add_feature(cfeature.COASTLINE); ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "        ax.gridlines(draw_labels=True)\n",
    "        units_label = f\"({var_data.units})\" if hasattr(var_data, 'units') else \"(units not specified)\"\n",
    "        plt.colorbar(mesh, ax=ax, orientation='vertical', label=f'{main_var_name} {units_label}')\n",
    "        ax.set_title(f'Input Data from {Path(filepath).name}\\n(First Time Step)')\n",
    "        plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "        logger.info(f\"Input data map saved to: {output_path}\")\n",
    "        plt.close(fig)\n",
    "\n",
    "def plot_output_bufr(filepath: str, output_path: str):\n",
    "    if not CARTOPY_AVAILABLE:\n",
    "        logger.warning(\"Cannot plot output map: Cartopy is not installed.\")\n",
    "        return\n",
    "        \n",
    "    logger.info(f\"Visualizing output BUFR file: {filepath}\")\n",
    "    lats, lons = [], []\n",
    "    msg_count = valid_points = 0\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while True:\n",
    "            # 1) grab the handle (or EOF)\n",
    "            try:\n",
    "                bufr = ec.codes_bufr_new_from_file(f)\n",
    "            except ec.CodesInternalError:\n",
    "                # true EOF for this API\n",
    "                break\n",
    "\n",
    "            if bufr is None:\n",
    "                # some bindings return None at EOF instead of raising\n",
    "                break\n",
    "\n",
    "            msg_count += 1\n",
    "            try:\n",
    "                # unpack and read array keys\n",
    "                ec.codes_set(bufr, 'unpack', 1)\n",
    "\n",
    "                try:\n",
    "                    lat_arr = ec.codes_get_array(bufr, 'latitude')\n",
    "                    lon_arr = ec.codes_get_array(bufr, 'longitude')\n",
    "                except ec.CodesInternalError:\n",
    "                    # fallback station keys\n",
    "                    lat_arr = ec.codes_get_array(bufr, 'stationLatitude')\n",
    "                    lon_arr = ec.codes_get_array(bufr, 'stationLongitude')\n",
    "\n",
    "                for lat, lon in zip(lat_arr, lon_arr):\n",
    "                    if np.isfinite(lat) and np.isfinite(lon):\n",
    "                        lats.append(lat)\n",
    "                        lons.append(lon)\n",
    "                        valid_points += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Skipping message #{msg_count}: {e}\")\n",
    "\n",
    "            finally:\n",
    "                # only release if we got a real handle\n",
    "                ec.codes_release(bufr)\n",
    "\n",
    "    logger.info(f\"Processed {msg_count} messages, found {valid_points} valid locations\")\n",
    "    if not lats:\n",
    "        logger.warning(\"No valid locations found – skipping plot.\")\n",
    "        return\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax  = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    ax.scatter(lons, lats,\n",
    "               marker='.', s=10,\n",
    "               transform=ccrs.PlateCarree(),\n",
    "               label='BUFR Obs')\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.gridlines(draw_labels=True)\n",
    "    ax.legend()\n",
    "    ax.set_global()\n",
    "    ax.set_title(\n",
    "        f'BUFR observation locations\\n'\n",
    "        f'({valid_points} points from {msg_count} messages)'\n",
    "    )\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    logger.info(f\"Output data map saved to: {output_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"ECMWF Workflow Visualization Tool\")\n",
    "    parser.add_argument('--netcdf_file', required=True, help='Path to the input NetCDF file')\n",
    "    parser.add_argument('--bufr_file', required=True, help='Path to the output BUFR file')\n",
    "    parser.add_argument('--output_dir', required=True, help='Directory to save the plots')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    plot_input_netcdf(args.netcdf_file, str(output_dir / 'input_data_map.png'))\n",
    "    plot_output_bufr(args.bufr_file, str(output_dir / 'output_locations_map.png'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b149dc-9cb6-4032-9e0e-23365749f971",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc91bf75-a871-41ab-97b7-d4997f7eb352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-08 02:10:01,516 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-08 02:10:01,516 - INFO - [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-08 02:10:01,516 - INFO - Preparing surface data for 2025-05-09\n",
      "2025-06-08 02:10:01,516 - INFO - Retrieving real surface data from CDS: 'reanalysis-era5-single-levels'\n",
      "2025-06-08 02:10:03,155 INFO Request ID is 09acad9d-6964-4783-9e11-ce1c0805f896\n",
      "2025-06-08 02:10:03,155 - INFO - Request ID is 09acad9d-6964-4783-9e11-ce1c0805f896\n",
      "2025-06-08 02:10:03,796 INFO status has been updated to accepted\n",
      "2025-06-08 02:10:03,796 - INFO - status has been updated to accepted\n",
      "2025-06-08 02:10:38,301 INFO status has been updated to successful\n",
      "2025-06-08 02:10:38,301 - INFO - status has been updated to successful\n",
      "2025-06-08 02:10:38,791 - INFO - Downloading https://object-store.os-api.cci2.ecmwf.int:443/cci2-prod-cache-2/2025-06-07/3b0c6d83fd80bce4f4f37f8b85775939.nc\n",
      "2025-06-08 02:10:41,285 - INFO - Successfully retrieved ERA5 surface data to test_data/surface_era5_20250509.nc\n",
      "Surface data file is ready: test_data/surface_era5_20250509.nc\n"
     ]
    }
   ],
   "source": [
    "!python ecmwf_data_retrieval.py --output test_data --type surface --real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1995009b-bb77-4a0a-8b29-68835e4ca242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-08 02:46:21,833 - INFO - Inferred base date from filename: 2025-05-09\n",
      "2025-06-08 02:46:23,418 - INFO - Extracted 3444 point observations from surface_era5_20250509.nc\n",
      "2025-06-08 02:46:23,988 - INFO - Successfully encoded 3444 BUFR messages to test_data/surface_era5_20250509.bufr\n",
      "Processing complete. Output: test_data/surface_era5_20250509.bufr\n"
     ]
    }
   ],
   "source": [
    "!python obs_pipeline.py -i test_data/surface_era5_20250509.nc -o test_data/surface_era5_20250509.bufr -t surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b4fa78-d9c5-4ac4-9002-22c7ccce398b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-08 02:46:40,612 - INFO - Visualizing input NetCDF file: test_data/surface_era5_20250509.nc\n",
      "2025-06-08 02:46:41,235 - INFO - Input data map saved to: test_data/input_data_map.png\n",
      "2025-06-08 02:46:41,235 - INFO - Visualizing output BUFR file: test_data/surface_era5_20250509.bufr\n",
      "2025-06-08 02:46:41,652 - INFO - Processed 3444 messages, found 3444 valid locations\n",
      "2025-06-08 02:46:41,922 - INFO - Output data map saved to: test_data/output_locations_map.png\n"
     ]
    }
   ],
   "source": [
    "!python visualize_data.py \\\n",
    "    --netcdf_file test_data/surface_era5_20250509.nc \\\n",
    "    --bufr_file test_data/surface_era5_20250509.bufr \\\n",
    "    --output_dir test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f316a4c-3003-48bc-8a2f-7128426985dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
